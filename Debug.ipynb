{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataload import dataload\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from torchvision import  utils\n",
    "\n",
    "from src import detect_faces, show_bboxes\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from module import video_model\n",
    "from constant import EMOTIPATH\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame_data=torch.rand((24,3,256,256))\n",
    "face_data=torch.rand((24,5,3,64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_embedded_data=torch.rand((32,24,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_embedded_data=torch.rand((32,24,5,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video_modeller(nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self,frame):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.en1=video_model.Encoder(num_frame=5,dim=100,model=None,isframe=0,label=256)\n",
    "        self.en2=video_model.Encoder(num_frame=frame,dim=256,model=None,isframe=1,label=3)\n",
    "        \n",
    "        self.fc1=nn.Linear(1000,256)\n",
    "        self.fc2=nn.Linear(512,256)\n",
    "    def face_embedder(self,x,t,encoder):\n",
    "        x1 = (encoder(x[:, t,:]))  # Pretrained_Densenet            \n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        \n",
    "        return x1\n",
    "\n",
    "    def check_raw(self,x,dim):\n",
    "        if x.shape[-1]==dim:\n",
    "            return False\n",
    "        return True \n",
    "        \n",
    "        return None\n",
    "    def stack_face_encoder(self,x):\n",
    "        ##Transformation to: Frames*Channel*width*height\n",
    "        cnn_embed_seq = []\n",
    "     \n",
    "        for t in range(x.size(1)):\n",
    "            with torch.no_grad():\n",
    "                x1=self.face_embedder(x,t,self.en1)\n",
    "                cnn_embed_seq.append(x1)            \n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        return cnn_embed_seq\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        initial_time=time.time()\n",
    "        if y is not None:\n",
    "            start=time.time()\n",
    "            frame_local=self.stack_face_encoder(y)\n",
    "            end=time.time()\n",
    "            print(end-start)\n",
    "        if x is not None:\n",
    "            frame_global=self.fc1(x)\n",
    "        if y is None:\n",
    "            return self.en2(frame_global)\n",
    "        if x is None:\n",
    "            return self.en2(frame_local)            \n",
    "            \n",
    "        \n",
    "        frames_embedding=torch.cat((frame_global,frame_local),2)\n",
    "        frames_embedding=self.fc2(frames_embedding)\n",
    "        vide_embed=self.en2(frames_embedding) \n",
    "        finish_time=time.time()\n",
    "        print(finish_time-initial_time)\n",
    "        return vide_embed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=Video_modeller(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04388427734375\n",
      "0.05385899543762207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3142, 0.3443, 0.3415],\n",
       "        [0.3145, 0.3448, 0.3408],\n",
       "        [0.3153, 0.3425, 0.3421],\n",
       "        [0.3143, 0.3434, 0.3423],\n",
       "        [0.3152, 0.3411, 0.3437],\n",
       "        [0.3158, 0.3433, 0.3409],\n",
       "        [0.3126, 0.3436, 0.3438],\n",
       "        [0.3139, 0.3448, 0.3413],\n",
       "        [0.3135, 0.3442, 0.3423],\n",
       "        [0.3160, 0.3432, 0.3408],\n",
       "        [0.3145, 0.3433, 0.3422],\n",
       "        [0.3160, 0.3427, 0.3413],\n",
       "        [0.3124, 0.3459, 0.3417],\n",
       "        [0.3151, 0.3447, 0.3402],\n",
       "        [0.3154, 0.3432, 0.3414],\n",
       "        [0.3135, 0.3433, 0.3432],\n",
       "        [0.3148, 0.3431, 0.3421],\n",
       "        [0.3137, 0.3454, 0.3409],\n",
       "        [0.3141, 0.3438, 0.3421],\n",
       "        [0.3148, 0.3439, 0.3413],\n",
       "        [0.3127, 0.3455, 0.3418],\n",
       "        [0.3140, 0.3435, 0.3425],\n",
       "        [0.3129, 0.3453, 0.3419],\n",
       "        [0.3129, 0.3440, 0.3431],\n",
       "        [0.3154, 0.3441, 0.3404],\n",
       "        [0.3153, 0.3436, 0.3411],\n",
       "        [0.3153, 0.3419, 0.3428],\n",
       "        [0.3146, 0.3446, 0.3408],\n",
       "        [0.3136, 0.3443, 0.3421],\n",
       "        [0.3145, 0.3425, 0.3430],\n",
       "        [0.3169, 0.3426, 0.3405],\n",
       "        [0.3137, 0.3423, 0.3440]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug(frame_embedded_data,face_embedded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=video_model.Encoder(num_frame=24,dim=512,model=None,isframe=0,label=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(torch.rand((1,24,512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Raw_Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-61a10aea5189>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mTrain_video\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEMOTIPATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"Train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mframe_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m24\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRaw_Data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain_label\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbase_path_v\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrain_video\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Raw_Data' is not defined"
     ]
    }
   ],
   "source": [
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_video=os.path.join(EMOTIPATH,\"Train\")\n",
    "frame_num=24\n",
    "train_data=Raw_Data(Train_label,base_path_v=Train_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Data():\n",
    "    def __init__(self,csv_file,base_path_v=None,):\n",
    "        \n",
    "        self.max_frame_num=24\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "\n",
    "        self._base_path_v=base_path_v\n",
    "  \n",
    "\n",
    "            \n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.Resize((256,256)),\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "        frame_list=os.listdir(folder_name)\n",
    "\n",
    "\n",
    "        return folder_name,frame_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[566]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter, attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best(num_frame,data):\n",
    "\n",
    "for index,(base_path,imglist) in enumerate(train_data):\n",
    "    confidence_score_list=[]\n",
    "    for img_frame in imglist:\n",
    "        \n",
    "        img=Image.open(os.path.join(base_path,img_frame))\n",
    "        total_confidence_score=sum(detect_faces(img)[0][0:5,-1])\n",
    "        confidence_score_list.append((total_confidence_score,img_frame))\n",
    "    confidence_score_list=sorted(confidence_score_list, key=itemgetter(0),reverse=True)\n",
    "    sorted(confidence_score_list[0:num_frame]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_score_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_score_list=sorted(confidence_score_list, key=itemgetter(0),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_score_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(confidence_score_list[0:5],key=itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=Image.open(os.path.join(train_data[0][0],train_data[0][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_score=sum(detect_faces(img)[0][0:5,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(confidence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces[0][0:5,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class cv_data(Dataset):\n",
    "    def __init__(self,csv_file,sub_csv_file=None,\n",
    "                 base_path_v=None,base_path_a=None,frame_num=16,strict_name=True,name_format=9,embedding=False):\n",
    "        \n",
    "        self.max_frame_num=24\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        if sub_csv_file is None:\n",
    "            self._table_embedding=None\n",
    "        else:\n",
    "            self._table_embedding=pd.read_csv(sub_csv_file)\n",
    "        self.frame_num = frame_num\n",
    "        self._base_path_v=base_path_v\n",
    "        self._base_path_a=base_path_a\n",
    "        self.embedding=embedding\n",
    "        if strict_name:\n",
    "            self.name_format=name_format\n",
    "            \n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "        first=True\n",
    "        audio_img=np.zeros((1,1))\n",
    "        labels = torch.from_numpy(np.array(self._table.Label[idx]))-1\n",
    "            \n",
    "        if self._table_embedding is not None:\n",
    "            temp_frame_embedding=torch.from_numpy(np.array(self._table_embedding.Embedding[idx].split(),dtype=float)).reshape((self.max_frame_num,-1))\n",
    "            frame_data=torch.empty(size=(self.frame_num,1000),dtype=torch.double)\n",
    "            if self.frame_num<25:\n",
    "                index=np.linspace(0,23,self.frame_num,dtype=int)\n",
    " \n",
    "                for i,copy in enumerate(index):\n",
    "                    frame_data[i]=temp_frame_embedding[copy]\n",
    "            return (frame_data,audio_img,labels)\n",
    "    \n",
    "\n",
    "            \n",
    "        frame_raw_list=os.listdir(folder_name)\n",
    "        frame_len=len(frame_raw_list)\n",
    "\n",
    "        frame_raw_list=sorted(frame_raw_list)\n",
    "\n",
    "        frame_list=[]\n",
    "        if frame_len<self.frame_num:\n",
    "            for index_0 in range(frame_len):\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_0])\n",
    "                \n",
    "                tempimg=cv2.imread(frame_path,cv2.IMREAD_COLOR)  \n",
    "                tempimg=cv2.resize(tempimg, (256,256), interpolation = cv2.INTER_AREA)\n",
    "               # frame_class=frame(tempimg,frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        else:    \n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                tempimg=cv2.imread(frame_path,cv2.IMREAD_COLOR)\n",
    "                tempimg=cv2.resize(tempimg, (256,256), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "                #frame_class=frame(tempimg,frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        frame_data=torch.stack(frame_list,dim=0)\n",
    "        return (frame_data,audio_img,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class pil_data(Dataset):\n",
    "    def __init__(self,csv_file,sub_csv_file=None,\n",
    "                 base_path_v=None,base_path_a=None,frame_num=16,strict_name=True,name_format=9,embedding=False):\n",
    "        \n",
    "        self.max_frame_num=24\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        if sub_csv_file is None:\n",
    "            self._table_embedding=None\n",
    "        else:\n",
    "            self._table_embedding=pd.read_csv(sub_csv_file)\n",
    "        self.frame_num = frame_num\n",
    "        self._base_path_v=base_path_v\n",
    "        self._base_path_a=base_path_a\n",
    "        self.embedding=embedding\n",
    "        if strict_name:\n",
    "            self.name_format=name_format\n",
    "            \n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.Resize((256,256)),\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "        first=True\n",
    "        audio_img=np.zeros((1,1))\n",
    "        labels = torch.from_numpy(np.array(self._table.Label[idx]))-1\n",
    "            \n",
    "        if self._table_embedding is not None:\n",
    "            temp_frame_embedding=torch.from_numpy(np.array(self._table_embedding.Embedding[idx].split(),dtype=float)).reshape((self.max_frame_num,-1))\n",
    "            frame_data=torch.empty(size=(self.frame_num,1000),dtype=torch.double)\n",
    "            if self.frame_num<25:\n",
    "                index=np.linspace(0,23,self.frame_num,dtype=int)\n",
    " \n",
    "                for i,copy in enumerate(index):\n",
    "                    frame_data[i]=temp_frame_embedding[copy]\n",
    "            return (frame_data,audio_img,labels)\n",
    "    \n",
    "        \n",
    "            \n",
    "        frame_raw_list=os.listdir(folder_name)\n",
    "        frame_len=len(frame_raw_list)\n",
    "\n",
    "        frame_raw_list=sorted(frame_raw_list)\n",
    "\n",
    "        frame_list=[]\n",
    "        if frame_len<self.frame_num:\n",
    "            for index_0 in range(frame_len):\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_0])\n",
    "                \n",
    "                tempimg=Image.open(frame_path)       \n",
    "               # frame_class=frame(tempimg,frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        else:    \n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                tempimg=Image.open(frame_path)\n",
    "                #frame_class=frame(tempimg,frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        frame_data=torch.stack(frame_list,dim=0)\n",
    "        return (frame_data,audio_img,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_video=os.path.join(EMOTIPATH,\"Train\")\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_video=os.path.join(EMOTIPATH,\"Val\")\n",
    "train_data_pil=pil_data(Train_label,base_path_v=Train_video,frame_num=24)\n",
    "train_dataloader_pil = DataLoader(train_data_pil, batch_size=24\n",
    "                       , num_workers=0,shuffle=False)                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cv=cv_data(Train_label,base_path_v=Train_video,frame_num=24)\n",
    "train_dataloader_cv = DataLoader(train_data_cv, batch_size=24\n",
    "                       , num_workers=0,shuffle=False)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "for i in enumerate(train_dataloader_pil):\n",
    "    break\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "for i in enumerate(train_dataloader_cv):\n",
    "    break\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame_Face_Data(Dataset):\n",
    "    def __init__(self,csv_file,sub_csv_file=None,base_path_v=None):\n",
    "        \n",
    "        self.max_frame_num=5\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        self._table_embedding=None\n",
    "        self.transform=transforms.Compose([\n",
    "                 #    transforms.Resize((256,256)),\n",
    "                     transforms.ToTensor(),   \n",
    "                    transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))\n",
    "        ])\n",
    "        self.endPad=self.transform(Image.new(mode='RGB', size=(64,64), color=0))\n",
    "        self._base_path_v=base_path_v\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "    def _stacking_face(self,face_num,faces):\n",
    "\n",
    "        face_v=torch.zeros((face_num,3,64,64))\n",
    "        for i in range(face_num):\n",
    "            face_v[i]=faces[:,:,i*64:i*64+64]\n",
    "        return face_v\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "\n",
    "        labels = torch.from_numpy(np.array(self._table.Label[idx]))-1\n",
    "            \n",
    "        if self._table_embedding is not None:\n",
    "            temp_frame_embedding=torch.from_numpy(np.array(self._table_embedding.Embedding[idx].split(),dtype=float)).reshape((self.max_frame_num,-1))\n",
    "            frame_data=torch.empty(size=(self.frame_num,1000),dtype=torch.double)\n",
    "            if self.frame_num<25:\n",
    "                index=np.linspace(0,23,self.frame_num,dtype=int)\n",
    " \n",
    "                for i,copy in enumerate(index):\n",
    "                    frame_data[i]=temp_frame_embedding[copy]\n",
    "            return (frame_data,audio_img,labels)\n",
    "    \n",
    "\n",
    "        face_len=5\n",
    "        face_raw_list=os.listdir(folder_name)\n",
    "        face_raw_list=sorted(face_raw_list)\n",
    "\n",
    "        face_list=[]\n",
    "        for index_0 in range(face_len):\n",
    "                face_path=os.path.join(folder_name,face_raw_list[index_0])               \n",
    "                tempimg=Image.open(face_path)       \n",
    "                face_list.append(self._stacking_face(face_len,self.transform(tempimg)))\n",
    "            \n",
    "\n",
    "        face_data=torch.stack(face_list,dim=0)\n",
    "            \n",
    "        return (face_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_face=os.path.join(EMOTIPATH,\"Face_Cropped\",\"Train\")\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_face=os.path.join(EMOTIPATH,\"Face_Cropped\",\"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=Frame_Face_Data(Train_label,Train_face,base_path_v=Train_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=test[0][0]*0.5+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_face(face_num,faces):\n",
    "\n",
    "    face_v=torch.zeros((face_num,3,64,64))\n",
    "    for i in range(face_num):\n",
    "        face_v[i]=faces[:,:,i*64:i*64+64]\n",
    "    return face_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    plt.subplot(5,1,i+1)\n",
    "    plt.imshow((trans((face_v[i]).squeeze(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_v[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

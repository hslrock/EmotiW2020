{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataload import dataload\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from torchvision import  utils\n",
    "\n",
    "from src import detect_faces, show_bboxes\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "from module import densenet,transformer\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "dense_pre=densenet.densenet121(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_frame=24,dim=1000,model=None,frame=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.label=3\n",
    "        if model is None:\n",
    "            self.embedder=densenet.densenet121(pretrained=False)\n",
    "            self.fc1=nn.Linear(1000,512)\n",
    "        else:\n",
    "            self.embedder=model\n",
    "        if frame==1:\n",
    "            self.fc1=nn.Linear(1000,256)\n",
    "        else:\n",
    "            self.fc1=nn.Linear(100,256)\n",
    "        self.dim_frame=dim\n",
    "        self.num_frame=num_frame\n",
    "        self.fc1=nn.Linear(1000,512)\n",
    "        self.fc2=nn.Linear(512, self.label)\n",
    "        \n",
    "        self.posencoding=transformer.PositionalEncoding(self.dim_frame,n_position=frame)\n",
    "        self.transformer=transformer.MultiHeadAttention(d_k=128,d_v=128,n_head=2,frame=frame)\n",
    "        \n",
    "    def img_embedder(self,x,t,embedder):\n",
    "        x1 = (embedder(x[:, t, :, :, :]))  # Pretrained_Densenet\n",
    "        x1 = x1.view(x1.size(0), -1)\n",
    "        \n",
    "        return x1\n",
    "    \n",
    "    \n",
    "    def check_raw(self,x,dim):\n",
    "        if x.shape[-1]==dim:\n",
    "            return False\n",
    "        return True \n",
    "        \n",
    "        return None\n",
    "    def stack_frame(self,x):\n",
    "        ##Transformation to: Frames*Channel*width*height\n",
    "\n",
    "        cnn_embed_seq = []\n",
    "     \n",
    "        for t in range(x.size(1)):\n",
    "            with torch.no_grad():\n",
    "                x1=self.img_embedder(x,t,self.embedder)\n",
    "                cnn_embed_seq.append(x1)            \n",
    "        cnn_embed_seq = torch.stack(cnn_embed_seq, dim=0).transpose_(0, 1)\n",
    "        \n",
    "        return cnn_embed_seq\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if  self.check_raw(x,self.dim_frame):\n",
    "            cnn_embed_seq=self.stack_frame(x)  \n",
    "            cnn_embed_seq=self.posencoding(cnn_embed_seq) \n",
    "           # return cnn_embed_seq\n",
    "        else:\n",
    "            cnn_embed_seq=x\n",
    "        ##Input to Transformer\n",
    "        \n",
    "        cnn_embed_seq=self.fc1(cnn_embed_seq) \n",
    "        ##Dimension Reduction\n",
    "        print(cnn_embed_seq.shape)\n",
    "        output=self.transformer(cnn_embed_seq,cnn_embed_seq,cnn_embed_seq)\n",
    "        \n",
    "        output,attention=output[0],output[1]\n",
    "        print(output.shape)\n",
    "        \n",
    "        output=(output.transpose(1,2))\n",
    "        output=self.fc2(F.relu(output))\n",
    "        #output=F.relu(output)\n",
    "        output=output.squeeze(1)\n",
    "        return F.softmax(output, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_model=Encoder(10,1000,dense_pre,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_frames=torch.rand(10,3,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 512])\n",
      "torch.Size([1, 512, 10])\n"
     ]
    }
   ],
   "source": [
    "output=frame_model(img_frames.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1009, 0.1061, 0.0938],\n",
       "         [0.0983, 0.0953, 0.1001],\n",
       "         [0.1021, 0.0975, 0.1032],\n",
       "         [0.0993, 0.1040, 0.0973],\n",
       "         [0.0994, 0.1049, 0.1034],\n",
       "         [0.1043, 0.0991, 0.0989],\n",
       "         [0.1016, 0.0935, 0.1003],\n",
       "         [0.0961, 0.0958, 0.1044],\n",
       "         [0.1024, 0.1033, 0.0978],\n",
       "         [0.0956, 0.1005, 0.1008]]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import video_model\n",
    "\n",
    "model=video_model.Video_Feature(joint=False,grayscale=False,classfication=True,frame=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=model.fc2(model.stack_frame(img_frames.unsqueeze(0)))\n",
    "\n",
    "model.transformer(x,x,x)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

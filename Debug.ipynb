{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataload import dataload\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from torchvision import  utils\n",
    "\n",
    "from src import detect_faces, show_bboxes\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from module import video_model\n",
    "from constant import EMOTIPATH,EMOTIFACEPATH\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constant import AFFECTNETPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "\n",
    "Train_face=os.path.join(EMOTIFACEPATH,\"Train\")\n",
    "Train_face_pt=os.path.join(EMOTIFACEPATH,\"pt\",\"Train\")\n",
    "\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_face=os.path.join(EMOTIFACEPATH,\"Valid\")\n",
    "Val_face_pt=os.path.join(EMOTIFACEPATH,\"pt\",\"Valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Face_Data(Dataset):\n",
    "    def __init__(self,csv_file,face_path=None,direct=True):\n",
    "        self.frame_num=25\n",
    "        self.max_frame_num=25\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "        self.face_path=face_path\n",
    "\n",
    "        self.direct=direct\n",
    "        \n",
    "    def _stacking_face(self,face_num,faces):\n",
    "\n",
    "        face_v=torch.zeros((face_num,3,64,64))\n",
    "        for i in range(face_num):\n",
    "            face_v[i]=faces[:,:,i*64:i*64+64]\n",
    "        return face_v\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self.face_path,self._table.Vid_name[idx])\n",
    "        if self.direct:\n",
    "            \n",
    "            face_path=folder_name\n",
    "            \n",
    "            frame_raw_list=os.listdir(folder_name)\n",
    "            frame_len=len(frame_raw_list)\n",
    "            frame_raw_list=sorted(frame_raw_list)\n",
    "            frame_list=[]\n",
    "\n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                face_pt=torch.load(frame_path)\n",
    "                face_pt=self._stacking_face(5,face_pt)\n",
    "                \n",
    "                frame_list.append(face_pt)\n",
    "            frame_data=torch.stack(frame_list,dim=0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            return (frame_data,self._table.Vid_name[idx])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            frame_raw_list=os.listdir(folder_name)\n",
    "            frame_len=len(frame_raw_list)\n",
    "\n",
    "            frame_raw_list=sorted(frame_raw_list)\n",
    "               # print(frame_raw_list)\n",
    "            frame_list=[]\n",
    "\n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                tempimg=Image.open(frame_path)\n",
    "                tempimg=self.transform(tempimg)\n",
    "                tempimg=self._stacking_face(5,tempimg)\n",
    "                frame_list.append(tempimg)\n",
    "                \n",
    "            while(len(frame_list)<self.frame_num):\n",
    "                frame_list.append(self.endPad)\n",
    "            frame_data=torch.stack(frame_list,dim=0)\n",
    "\n",
    "            return (frame_data,self._table.Vid_name[idx])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=Face_Data(Train_label,Train_face,direct=False)\n",
    "valid_data=Face_Data(Val_labels,Val_face,direct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath=os.path.join(EMOTIFACEPATH,\"pt_stacked\")\n",
    "if not os.path.isdir(savepath):\n",
    "    os.mkdir(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_save_path=os.path.join(savepath,\"Train\")\n",
    "valid_save_path=os.path.join(savepath,\"Valid\")\n",
    "if not os.path.isdir(train_save_path):\n",
    "    os.mkdir(train_save_path)\n",
    "if not os.path.isdir(valid_save_path):\n",
    "    os.mkdir(valid_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 5, 3, 64, 64])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd4d5fe0b114f50bbdc20e51351d817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "2661",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-9e993354161c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_save_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-3574da179650>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfolder_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVid_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 2661"
     ]
    }
   ],
   "source": [
    "for data in tqdm(train_data):\n",
    "    filename=data[1]+\".pt\"\n",
    "    torch.save(data[0],os.path.join(train_save_path,filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2_1'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FF181D76D50>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans=transforms.ToPILImage()\n",
    "trans(dataset[0][0][22][0]*0.5+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FF18231AB50>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans(dataset1[0][0][22][0]*0.5+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  0,  0, ..., 24, 24, 24]),\n",
       " array([0, 0, 0, ..., 2, 2, 2]),\n",
       " array([ 0,  0,  0, ..., 63, 63, 63]),\n",
       " array([  0,   1,   5, ..., 189, 190, 191]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(dataset[0][0]!=dataset1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Data():\n",
    "    def __init__(self,csv_file,base_path_v=None,):\n",
    "        \n",
    "        self.max_frame_num=24\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "\n",
    "        self._base_path_v=base_path_v\n",
    "  \n",
    "\n",
    "            \n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.Resize((256,256)),\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "        frame_list=os.listdir(folder_name)\n",
    "\n",
    "\n",
    "        return folder_name,frame_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[566]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter, attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_best(num_frame,data):\n",
    "\n",
    "for index,(base_path,imglist) in enumerate(train_data):\n",
    "    confidence_score_list=[]\n",
    "    for img_frame in imglist:\n",
    "        \n",
    "        img=Image.open(os.path.join(base_path,img_frame))\n",
    "        total_confidence_score=sum(detect_faces(img)[0][0:5,-1])\n",
    "        confidence_score_list.append((total_confidence_score,img_frame))\n",
    "    confidence_score_list=sorted(confidence_score_list, key=itemgetter(0),reverse=True)\n",
    "    sorted(confidence_score_list[0:num_frame]\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_score_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_score_list=sorted(confidence_score_list, key=itemgetter(0),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_score_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(confidence_score_list[0:5],key=itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=Image.open(os.path.join(train_data[0][0],train_data[0][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_score=sum(detect_faces(img)[0][0:5,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(confidence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces[0][0:5,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class cv_data(Dataset):\n",
    "    def __init__(self,csv_file,sub_csv_file=None,\n",
    "                 base_path_v=None,base_path_a=None,frame_num=16,strict_name=True,name_format=9,embedding=False):\n",
    "        \n",
    "        self.max_frame_num=24\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        if sub_csv_file is None:\n",
    "            self._table_embedding=None\n",
    "        else:\n",
    "            self._table_embedding=pd.read_csv(sub_csv_file)\n",
    "        self.frame_num = frame_num\n",
    "        self._base_path_v=base_path_v\n",
    "        self._base_path_a=base_path_a\n",
    "        self.embedding=embedding\n",
    "        if strict_name:\n",
    "            self.name_format=name_format\n",
    "            \n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "        first=True\n",
    "        audio_img=np.zeros((1,1))\n",
    "        labels = torch.from_numpy(np.array(self._table.Label[idx]))-1\n",
    "            \n",
    "        if self._table_embedding is not None:\n",
    "            temp_frame_embedding=torch.from_numpy(np.array(self._table_embedding.Embedding[idx].split(),dtype=float)).reshape((self.max_frame_num,-1))\n",
    "            frame_data=torch.empty(size=(self.frame_num,1000),dtype=torch.double)\n",
    "            if self.frame_num<self.max_frame_num:\n",
    "                index=np.linspace(0,self.max_frame_num-1,self.frame_num,dtype=int)\n",
    " \n",
    "                for i,copy in enumerate(index):\n",
    "                    frame_data[i]=temp_frame_embedding[copy]\n",
    "            return (frame_data,audio_img,labels)\n",
    "    \n",
    "\n",
    "            \n",
    "        frame_raw_list=os.listdir(folder_name)\n",
    "        frame_len=len(frame_raw_list)\n",
    "\n",
    "        frame_raw_list=sorted(frame_raw_list)\n",
    "\n",
    "        frame_list=[]\n",
    "        if frame_len<self.frame_num:\n",
    "            for index_0 in range(frame_len):\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_0])\n",
    "                \n",
    "                tempimg=cv2.imread(frame_path,cv2.IMREAD_COLOR)  \n",
    "                tempimg=cv2.resize(tempimg, (256,256), interpolation = cv2.INTER_AREA)\n",
    "               # frame_class=frame(tempimg,frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        else:    \n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                tempimg=cv2.imread(frame_path,cv2.IMREAD_COLOR)\n",
    "                tempimg=cv2.resize(tempimg, (256,256), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "                #frame_class=frame(tempimg,frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        frame_data=torch.stack(frame_list,dim=0)\n",
    "        return (frame_data,audio_img,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "class pil_data(Dataset):\n",
    "    def __init__(self,csv_file,sub_csv_file=None,\n",
    "                 base_path_v=None,base_path_a=None,frame_num=16,strict_name=True,name_format=9,embedding=False):\n",
    "        \n",
    "        self.max_frame_num=24\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        if sub_csv_file is None:\n",
    "            self._table_embedding=None\n",
    "        else:\n",
    "            self._table_embedding=pd.read_csv(sub_csv_file)\n",
    "        self.frame_num = frame_num\n",
    "        self._base_path_v=base_path_v\n",
    "        self._base_path_a=base_path_a\n",
    "        self.embedding=embedding\n",
    "        if strict_name:\n",
    "            self.name_format=name_format\n",
    "            \n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.Resize((256,256)),\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "        first=True\n",
    "        audio_img=np.zeros((1,1))\n",
    "        labels = torch.from_numpy(np.array(self._table.Label[idx]))-1\n",
    "            \n",
    "        if self._table_embedding is not None:\n",
    "            temp_frame_embedding=torch.from_numpy(np.array(self._table_embedding.Embedding[idx].split(),dtype=float)).reshape((self.max_frame_num,-1))\n",
    "            frame_data=torch.empty(size=(self.frame_num,1000),dtype=torch.double)\n",
    "            if self.frame_num<25:\n",
    "                index=np.linspace(0,23,self.frame_num,dtype=int)\n",
    " \n",
    "                for i,copy in enumerate(index):\n",
    "                    frame_data[i]=temp_frame_embedding[copy]\n",
    "            return (frame_data,audio_img,labels)\n",
    "    \n",
    "        \n",
    "            \n",
    "        frame_raw_list=os.listdir(folder_name)\n",
    "        frame_len=len(frame_raw_list)\n",
    "\n",
    "        frame_raw_list=sorted(frame_raw_list)\n",
    "\n",
    "        frame_list=[]\n",
    "        if frame_len<self.frame_num:\n",
    "            for index_0 in range(frame_len):\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_0])\n",
    "                \n",
    "                tempimg=Image.open(frame_path)       \n",
    "               # frame_class=frame(tempimg,frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        else:    \n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                tempimg=Image.open(frame_path)\n",
    "                #frame_class=frame(tempimg,frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        frame_data=torch.stack(frame_list,dim=0)\n",
    "        return (frame_data,audio_img,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_video=os.path.join(EMOTIPATH,\"Train\")\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_video=os.path.join(EMOTIPATH,\"Val\")\n",
    "train_data_pil=pil_data(Train_label,base_path_v=Train_video,frame_num=24)\n",
    "train_dataloader_pil = DataLoader(train_data_pil, batch_size=16\n",
    "                       , num_workers=0,shuffle=False)                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cv=cv_data(Train_label,base_path_v=Train_video,frame_num=24)\n",
    "train_dataloader_cv = DataLoader(train_data_cv, batch_size=16\n",
    "                       , num_workers=0,shuffle=False)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "for i in enumerate(train_dataloader_pil):\n",
    "    break\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "for i in enumerate(train_dataloader_cv):\n",
    "    break\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Frame_Face_Data(Dataset):\n",
    "    def __init__(self,csv_file,sub_csv_file=None,base_path_v=None):\n",
    "        \n",
    "        self.max_frame_num=5\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        self._table_embedding=None\n",
    "        self.transform=transforms.Compose([\n",
    "                 #    transforms.Resize((256,256)),\n",
    "                     transforms.ToTensor(),   \n",
    "                    transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))\n",
    "        ])\n",
    "        self.endPad=self.transform(Image.new(mode='RGB', size=(64,64), color=0))\n",
    "        self._base_path_v=base_path_v\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "    def _stacking_face(self,face_num,faces):\n",
    "\n",
    "        face_v=torch.zeros((face_num,3,64,64))\n",
    "        for i in range(face_num):\n",
    "            face_v[i]=faces[:,:,i*64:i*64+64]\n",
    "        return face_v\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "\n",
    "        labels = torch.from_numpy(np.array(self._table.Label[idx]))-1\n",
    "            \n",
    "        if self._table_embedding is not None:\n",
    "            temp_frame_embedding=torch.from_numpy(np.array(self._table_embedding.Embedding[idx].split(),dtype=float)).reshape((self.max_frame_num,-1))\n",
    "            frame_data=torch.empty(size=(self.frame_num,1000),dtype=torch.double)\n",
    "            if self.frame_num<25:\n",
    "                index=np.linspace(0,23,self.frame_num,dtype=int)\n",
    " \n",
    "                for i,copy in enumerate(index):\n",
    "                    frame_data[i]=temp_frame_embedding[copy]\n",
    "            return (frame_data,audio_img,labels)\n",
    "    \n",
    "\n",
    "        face_len=5\n",
    "        face_raw_list=os.listdir(folder_name)\n",
    "        face_raw_list=sorted(face_raw_list)\n",
    "\n",
    "        face_list=[]\n",
    "        for index_0 in range(face_len):\n",
    "                face_path=os.path.join(folder_name,face_raw_list[index_0])               \n",
    "                tempimg=Image.open(face_path)       \n",
    "                face_list.append(self._stacking_face(face_len,self.transform(tempimg)))\n",
    "            \n",
    "\n",
    "        face_data=torch.stack(face_list,dim=0)\n",
    "            \n",
    "        return (face_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_face=os.path.join(EMOTIPATH,\"Face_Cropped\",\"Train\")\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_face=os.path.join(EMOTIPATH,\"Face_Cropped\",\"Val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=Frame_Face_Data(Train_label,Train_face,base_path_v=Train_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=test[0][0]*0.5+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacking_face(face_num,faces):\n",
    "\n",
    "    face_v=torch.zeros((face_num,3,64,64))\n",
    "    for i in range(face_num):\n",
    "        face_v[i]=faces[:,:,i*64:i*64+64]\n",
    "    return face_v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    plt.subplot(5,1,i+1)\n",
    "    plt.imshow((trans((face_v[i]).squeeze(0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_v[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

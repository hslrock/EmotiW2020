{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataload import dataload\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from torchvision import  utils\n",
    "from constant import EMOTIPATH\n",
    "from src import detect_faces, show_bboxes\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_base_path=\"../../../hdd/Dataset/Dataset/Face_Cropped2/\"\n",
    "\n",
    "\n",
    "<!-- os.mkdir(new_base_path)\n",
    "\n",
    "os.mkdir(os.path.join(new_base_path,\"Train\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\",\"Train\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"Valid\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\",\"Valid\")   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Data():\n",
    "    def __init__(self,csv_file,sub_csv_file=None,\n",
    "                 base_path_v=None,frame_num=25,embedding=False):\n",
    "        \n",
    "        self.max_frame_num=25\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        if sub_csv_file is None:\n",
    "            self._table_embedding=None\n",
    "        else:\n",
    "            self._table_embedding=pd.read_csv(sub_csv_file)\n",
    "        self.frame_num = frame_num\n",
    "        self._base_path_v=base_path_v\n",
    "        self.embedding=embedding\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.Resize((256,256)),\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "        self.endPad=self.transform(Image.new(mode='RGB', size=(256,256), color=0))    \n",
    "\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx])\n",
    "        first=True\n",
    "            \n",
    "        if self._table_embedding is not None:\n",
    "            temp_frame_embedding=torch.from_numpy(np.array(self._table_embedding.Embedding[idx].split(),dtype=float)).reshape((self.max_frame_num,-1))\n",
    "            frame_data=torch.empty(size=(self.frame_num,1000),dtype=torch.double)\n",
    "            if self.frame_num<25:\n",
    "                index=np.linspace(0,23,self.frame_num,dtype=int)\n",
    " \n",
    "                for i,copy in enumerate(index):\n",
    "                    frame_data[i]=temp_frame_embedding[copy]\n",
    "            return (frame_data,audio_img,labels)\n",
    "    \n",
    "\n",
    "            \n",
    "        frame_raw_list=os.listdir(folder_name)\n",
    "        frame_len=len(frame_raw_list)\n",
    "\n",
    "        frame_raw_list=sorted(frame_raw_list)\n",
    "       # print(frame_raw_list)\n",
    "        frame_list=[]\n",
    "        \n",
    "        if frame_len<self.frame_num:\n",
    "            for index_0 in range(frame_len):\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_0])               \n",
    "                tempimg=Image.open(frame_path)       \n",
    "                frame_list.append(self.transform(tempimg))\n",
    "            \n",
    "        else:    \n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                tempimg=Image.open(frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        while(len(frame_list)<self.frame_num):\n",
    "            frame_list.append(self.endPad)\n",
    "        frame_data=torch.stack(frame_list,dim=0)\n",
    "            \n",
    "        return (folder_name,frame_data,frame_raw_list)\n",
    "    \n",
    "\n",
    "def face_extraction(img,max_number):\n",
    "        def toonePIL(x,max_number):\n",
    "            new_im = Image.new('RGB', (64*max_number,64))\n",
    "\n",
    "            x_offset = 0\n",
    "            for im in x:\n",
    "                new_im.paste(im, (x_offset,0))\n",
    "                x_offset += im.size[0]\n",
    "            return new_im\n",
    "\n",
    "        bounding_boxes, landmarks = detect_faces(img)\n",
    "        img_list=[]\n",
    "        transform=transforms.Resize((64,64))\n",
    "        transform2=transforms.Compose([\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "        for box_index,(left,right,up,bottom,_) in enumerate(bounding_boxes):\n",
    "            cropped_img=img.crop((left,right,up,bottom))\n",
    "            img_list.append(transform(cropped_img))\n",
    "            \n",
    "            if len(img_list)==max_number:\n",
    "                break\n",
    "        while len(img_list) !=max_number:\n",
    "            END_PAD= Image.new(mode = \"RGB\", size = (64, 64), color =(0, 0, 0))\n",
    "            img_list.append(END_PAD)\n",
    "        img=toonePIL(img_list,max_number)\n",
    "        pt=transform2(img)\n",
    "        return img,pt\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_video=os.path.join(EMOTIPATH,\"Train\")\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_video=os.path.join(EMOTIPATH,\"Val\")\n",
    "train_data=Raw_Data(Train_label,base_path_v=Train_video,frame_num=25)\n",
    "valid_data=Raw_Data(Val_labels,base_path_v=Val_video,frame_num=25)\n",
    "\n",
    "\n",
    "save_image_path=os.path.join(new_base_path,\"Train\",os.path.basename(video_name))\n",
    "save_pt_path=os.path.join(new_base_path,\"pt\",\"Train\",os.path.basename(video_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##os.mkdir('../../../hdd/Dataset/Dataset/Face_Cropped/Val')\n",
    "\n",
    "def create_face(data,save_pt_path,save_image_path:)\n",
    "    for index in tqdm(data):\n",
    "        video_name,_,frame_list=index\n",
    "        for frame_name in frame_list:\n",
    "\n",
    "            path=os.path.join(video_name,frame_name)\n",
    "            img=Image.open(path)\n",
    "            try:\n",
    "                face_cropped,pt=face_extraction(img,5)\n",
    "            except:\n",
    "                face_cropped=Image.new(mode='RGB', size=(320,64), color=0)\n",
    "                pt=transform2(face_cropped)\n",
    "                print(video_name,frame_name)\n",
    "                continue\n",
    "            \n",
    "            if not os.path.isdir(save_image_path):\n",
    "                os.mkdir(save_image_path)\n",
    "            if not os.path.isdir(save_pt_path):\n",
    "                os.mkdir(save_pt_path)\n",
    "            face_cropped.save((os.path.join(save_image_path,frame_name)))\n",
    "            torch.save(pt,(os.path.join(save_pt_path,os.path.splitext(frame_name)[0]+'.pt')))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

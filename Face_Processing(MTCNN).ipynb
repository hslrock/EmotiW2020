{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataload import dataload\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from torchvision import  utils\n",
    "from constant import EMOTIPATH,EMOTIFACEPATH\n",
    "from src import detect_faces, show_bboxes\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_base_path=\"../../../hdd/Dataset/Dataset/Face_Cropped2/\"\n",
    "\n",
    "\n",
    "<!-- os.mkdir(new_base_path)\n",
    "\n",
    "os.mkdir(os.path.join(new_base_path,\"Train\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\",\"Train\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"Valid\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\",\"Valid\")   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Data():\n",
    "    def __init__(self,csv_file,sub_csv_file=None,\n",
    "                 base_path_v=None,frame_num=25,embedding=False):\n",
    "        \n",
    "        self.max_frame_num=25\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        if sub_csv_file is None:\n",
    "            self._table_embedding=None\n",
    "        else:\n",
    "            self._table_embedding=pd.read_csv(sub_csv_file)\n",
    "        self.frame_num = frame_num\n",
    "        self._base_path_v=base_path_v\n",
    "        self.embedding=embedding\n",
    "        self.transform=transforms.Compose([\n",
    "                    transforms.Resize((256,256)),\n",
    "            \n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "        self.endPad=self.transform(Image.new(mode='RGB', size=(256,256), color=0))    \n",
    "\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx]) #base path\n",
    "        first=True          \n",
    "        frame_raw_list=os.listdir(folder_name)\n",
    "        frame_len=len(frame_raw_list)\n",
    "\n",
    "        frame_raw_list=sorted(frame_raw_list)\n",
    "       # print(frame_raw_list)\n",
    "        frame_list=[]\n",
    "        \n",
    "        if frame_len<self.frame_num:\n",
    "            for index_0 in range(frame_len):\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_0])               \n",
    "                tempimg=Image.open(frame_path)       \n",
    "                frame_list.append(self.transform(tempimg))\n",
    "            \n",
    "        else:    \n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                tempimg=Image.open(frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        while(len(frame_list)<self.frame_num):\n",
    "            frame_list.append(self.endPad)\n",
    "        frame_data=torch.stack(frame_list,dim=0)\n",
    "            \n",
    "        return (folder_name,frame_data,frame_raw_list)\n",
    "    \n",
    "\n",
    "def face_extraction(img,max_number):\n",
    "        def toonePIL(x,max_number):\n",
    "            new_im = Image.new('RGB', (64*max_number,64))\n",
    "\n",
    "            x_offset = 0\n",
    "            for im in x:\n",
    "                new_im.paste(im, (x_offset,0))\n",
    "                x_offset += im.size[0]\n",
    "            return new_im\n",
    "\n",
    "        bounding_boxes, landmarks = detect_faces(img)\n",
    "        img_list=[]\n",
    "        stacked_img=torch.zeros((5,3,64,64))\n",
    "        transform=transforms.Resize((64,64))\n",
    "        transform2=transforms.Compose([\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "        for box_index,(left,right,up,bottom,_) in enumerate(bounding_boxes):\n",
    "            cropped_img=img.crop((left,right,up,bottom))\n",
    "            cropped_img=transform(cropped_img)\n",
    "            img_list.append(cropped_img)\n",
    "            \n",
    "            stacked_img[box_index]=transform2(cropped_img)\n",
    "            if len(img_list)==max_number:\n",
    "                break\n",
    "        while len(img_list) !=max_number:\n",
    "            END_PAD= Image.new(mode = \"RGB\", size = (64, 64), color =(0, 0, 0))\n",
    "            img_list.append(END_PAD)\n",
    "        img=toonePIL(img_list,max_number)\n",
    "        pt=transform2(img)\n",
    "        return img,pt,stacked_img\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "Test_label=os.path.join(EMOTIPATH,\"Test_Data.csv\")\n",
    "Test_video=os.path.join(EMOTIPATH,\"Test\")\n",
    "Test_Data=Raw_Data(Test_label,base_path_v=Test_video,frame_num=25)\n",
    "\n",
    "\n",
    "#save_image_path=os.path.join(new_base_path,\"Test\",os.path.basename(video_name))\n",
    "#save_pt_path=os.path.join(new_base_path,\"pt\",\"Test\",os.path.basename(video_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660e5e1a9b4243f689ed3abf64b5f821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=756.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for index in tqdm(Test_Data):\n",
    "    video_name,_,frame_list=index\n",
    "    frame_size=len(frame_list)\n",
    "    save_stacked_pt_path=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Test\")\n",
    "    if not os.path.isdir(save_stacked_pt_path):\n",
    "            os.mkdir(save_stacked_pt_path)\n",
    "    stacked_face=torch.empty(size=(frame_size,5,3,64,64))\n",
    "    for frame_num,frame_name in enumerate(frame_list):\n",
    "        \n",
    "        path=os.path.join(video_name,frame_name)\n",
    "        img=Image.open(path)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c=face_extraction(img,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 64, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(Test_Data):\n",
    "#     video_name=os.path.basename(i[0])\n",
    "#     file_name=video_name+'.pt'\n",
    "#     save_path=os.path.join(EMOTIPATH,\"pt\",\"Test\")\n",
    "#     if not os.path.isdir(save_path):\n",
    "#             os.mkdir(save_path)\n",
    "\n",
    "#     torch.save(i[1], os.path.join(save_path,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.basename(Test_Data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##os.mkdir('../../../hdd/Dataset/Dataset/Face_Cropped/Val')\n",
    "\n",
    "def create_face(data,save_pt_path,save_image_path):\n",
    "    for index in tqdm(data):\n",
    "        video_name,_,frame_list=index\n",
    "        \n",
    "        for frame_name in frame_list:\n",
    "        \n",
    "            path=os.path.join(video_name,frame_name)\n",
    "            img=Image.open(path)\n",
    "            try:\n",
    "                face_cropped,pt=face_extraction(img,5)\n",
    "            except:\n",
    "                face_cropped=Image.new(mode='RGB', size=(320,64), color=0)\n",
    "                pt=transform2(face_cropped)\n",
    "                print(video_name,frame_name)\n",
    "                continue\n",
    "            \n",
    "            if not os.path.isdir(save_image_path):\n",
    "                os.mkdir(save_image_path)\n",
    "            if not os.path.isdir(save_pt_path):\n",
    "                os.mkdir(save_pt_path)\n",
    "            face_cropped.save((os.path.join(save_image_path,frame_name)))\n",
    "            torch.save(pt,(os.path.join(save_pt_path,os.path.splitext(frame_name)[0]+'.pt')))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08700c788b3d498eb38dc0d97d4b8015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=756.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../../hdd/Dataset/EmotiW/images/Test/Test_57 000023.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_197 000020.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_197 000021.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_197 000022.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_197 000023.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_197 000024.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_197 000025.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_483 000020.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_483 000025.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_510 000020.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_510 000022.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_569 000019.jpg\n",
      "../../../hdd/Dataset/EmotiW/images/Test/Test_569 000020.jpg\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "756",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d808ef43d6dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m              \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m              transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTest_Data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvideo_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mframe_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1095\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-c5c18f29377e>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mfolder_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_path_v\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVid_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#base path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mframe_raw_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 756"
     ]
    }
   ],
   "source": [
    "transform2=transforms.Compose([\n",
    "             transforms.ToTensor(),   \n",
    "             transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "for index in tqdm(Test_Data):\n",
    "    video_name,_,frame_list=index\n",
    "    frame_size=len(frame_list)\n",
    "    save_stacked_pt_path=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Test\")\n",
    "    if not os.path.isdir(save_stacked_pt_path):\n",
    "            os.mkdir(save_stacked_pt_path)\n",
    "    stacked_face=torch.empty(size=(frame_size,5,3,64,64))\n",
    "    for frame_num,frame_name in enumerate(frame_list):\n",
    "        \n",
    "        path=os.path.join(video_name,frame_name)\n",
    "        img=Image.open(path)\n",
    "        try:\n",
    "            face_cropped,pt,stacked_img=face_extraction(img,5)\n",
    "        except:\n",
    "            stacked_img=torch.zeros((5,3,64,64))\n",
    "            face_cropped=Image.new(mode='RGB', size=(320,64), color=0)\n",
    "            pt=transform2(face_cropped)\n",
    "            print(video_name,frame_name)\n",
    "            continue\n",
    "        stacked_face[frame_num]=stacked_img\n",
    "        save_image_path=os.path.join(EMOTIFACEPATH,\"Test\",os.path.basename(video_name))\n",
    "        save_pt_path=os.path.join(EMOTIFACEPATH,\"pt\",\"Test\",os.path.basename(video_name))\n",
    "        \n",
    "        if not os.path.isdir(save_image_path):\n",
    "            os.mkdir(save_image_path)\n",
    "        if not os.path.isdir(save_pt_path):\n",
    "            os.mkdir(save_pt_path)\n",
    "        face_cropped.save((os.path.join(save_image_path,frame_name)))\n",
    "        torch.save(pt,(os.path.join(save_pt_path,os.path.splitext(frame_name)[0]+'.pt')))\n",
    "    torch.save(stacked_face,(os.path.join(save_stacked_pt_path,os.path.basename(video_name)+'.pt')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from constant import AFFECTNETPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crops',\n",
       " 'features.tar',\n",
       " 'affectnet_train',\n",
       " 'validation.mod.pkl',\n",
       " 'training.mod.pkl',\n",
       " 'cropped_Annotated',\n",
       " 'training.csv',\n",
       " 'validation.csv',\n",
       " 'affectnet_val']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../../../hdd/Dataset/AffectNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file,base_path,transform=None):\n",
    "        self.fields = ['subDirectory_filePath', 'expression','valence','arousal']\n",
    "        self._table = pd.read_csv(csv_file,usecols=self.fields)\n",
    "        \n",
    "        self._table=self._table[self._table['expression'] <9]\n",
    "\n",
    "        self._table=self._table.reset_index(drop=True)\n",
    "\n",
    "        self._base_path=base_path\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.Resize((64,64)),\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path,self._table.subDirectory_filePath[idx])\n",
    "        img=Image.open(folder_name)\n",
    "        img=self.transform(img)\n",
    "        valence = torch.from_numpy(np.array(self._table.valence[idx]))\n",
    "        arousal = torch.from_numpy(np.array(self._table.arousal[idx]))\n",
    "        return img,valence.float(),arousal.float()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Affect_Train=ImageDataset('training.csv',AFFECTNETPATH)\n",
    "Affect_Valid=ImageDataset('validation.csv',AFFECTNETPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320740"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Affect_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Affect_Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    for i,file in enumerate(Affect_Valid):\n",
    "        if i%10000==0:\n",
    "            print(file[1])\n",
    "except FileNotFoundError:\n",
    "    \n",
    "    print(\"Error Occurs in:\", i)\n",
    "    pass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    for i,file in enumerate(Affect_Train):\n",
    "        if i%10000==0:\n",
    "            print(file[1])\n",
    "except FileNotFoundError:\n",
    "    \n",
    "    print(\"Error Occurs in:\", i)\n",
    "    pass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3569,  0.4510,  0.4588,  ..., -0.9686, -0.9608, -0.9686],\n",
       "         [ 0.4118,  0.4745,  0.4745,  ..., -0.9373, -0.9529, -0.9608],\n",
       "         [ 0.4353,  0.4824,  0.4902,  ..., -0.9294, -0.9294, -0.9216],\n",
       "         ...,\n",
       "         [ 0.7333,  0.7412,  0.7255,  ...,  0.5451,  0.5137,  0.4824],\n",
       "         [ 0.7255,  0.7176,  0.6863,  ...,  0.4667,  0.4353,  0.4118],\n",
       "         [ 0.7098,  0.7176,  0.7176,  ...,  0.4196,  0.4196,  0.3882]],\n",
       "\n",
       "        [[ 0.1059,  0.2235,  0.2314,  ..., -0.9765, -0.9686, -0.9765],\n",
       "         [ 0.1451,  0.2471,  0.2471,  ..., -0.9451, -0.9608, -0.9686],\n",
       "         [ 0.1529,  0.2471,  0.2549,  ..., -0.9373, -0.9373, -0.9294],\n",
       "         ...,\n",
       "         [ 0.3961,  0.4039,  0.3882,  ...,  0.1529,  0.1216,  0.0902],\n",
       "         [ 0.3882,  0.3804,  0.3490,  ...,  0.0745,  0.0353,  0.0118],\n",
       "         [ 0.3725,  0.3804,  0.3804,  ...,  0.0196,  0.0196, -0.0118]],\n",
       "\n",
       "        [[-0.2235,  0.0118,  0.0667,  ..., -0.9294, -0.9216, -0.9294],\n",
       "         [-0.1373,  0.0588,  0.0902,  ..., -0.8980, -0.9137, -0.9216],\n",
       "         [-0.0902,  0.0824,  0.0980,  ..., -0.8902, -0.8902, -0.8824],\n",
       "         ...,\n",
       "         [ 0.1216,  0.1294,  0.1137,  ..., -0.0667, -0.0980, -0.1294],\n",
       "         [ 0.1137,  0.1059,  0.0745,  ..., -0.1373, -0.1765, -0.2000],\n",
       "         [ 0.0980,  0.1059,  0.1059,  ..., -0.1922, -0.1922, -0.2235]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Affect_Train[243557][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(Affect_Train, batch_size=512\n",
    "                       , num_workers=0,shuffle=True)\n",
    "\n",
    "valid_dataloader = DataLoader(Affect_Valid, batch_size=512\n",
    "                       , num_workers=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Affect_Train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import face_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=face_feature.Face_Feature()\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3269]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.3715]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Affect_Train[0][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([512])) that is different to the input size (torch.Size([512, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  1 / 627\n",
      "MSE loss:  0.5999425649642944\n",
      "Batch:  501 / 627\n",
      "MSE loss:  0.5151660442352295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([228])) that is different to the input size (torch.Size([228, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([404])) that is different to the input size (torch.Size([404, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Face_Feature. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ResNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Conv2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BatchNorm2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ReLU. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MaxPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type BasicBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type AdaptiveAvgPool2d. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Tanh. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValidationLoss:  0.6498083538479276\n",
      "Training Epoch:  1 \n",
      "\n",
      "Batch:  1 / 627\n",
      "MSE loss:  0.5167564153671265\n",
      "Batch:  501 / 627\n",
      "MSE loss:  0.4954649806022644\n",
      "ValidationLoss:  0.770111428366767\n",
      "Training Epoch:  2 \n",
      "\n",
      "Batch:  1 / 627\n",
      "MSE loss:  0.5344341993331909\n"
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "num_epochs=50\n",
    "\n",
    "model.train()\n",
    "for epochs in range(0,num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "   \n",
    "    for i_batch, (img,valence,arousal) in enumerate(train_dataloader):\n",
    "        \n",
    "        batch_size=img.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        img=img.to(device)\n",
    "        val_output,arou_output=model(img)\n",
    "        loss1=loss_func(val_output,valence.to(device))\n",
    "        loss2=loss_func(arou_output,valence.to(device))\n",
    "        loss=loss1+loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch%500==0:\n",
    "            print(\"Batch: \",i_batch+1,\"/\",len(train_dataloader))\n",
    "            print(\"MSE loss: \", loss.item())\n",
    "            train_loss.append(loss.item())\n",
    "    model.eval()\n",
    "    avg_loss=1e6\n",
    "    temp_loss=0\n",
    "    for i_batch, (img,valence,arousal) in enumerate(valid_dataloader):\n",
    "        \n",
    "        batch_size=img.size(0)\n",
    "\n",
    "        img=img.to(device)\n",
    "        val_output,arou_output=model(img)\n",
    "        loss1=loss_func(val_output,valence.to(device))\n",
    "        loss2=loss_func(arou_output,valence.to(device))\n",
    "        loss=loss1+loss2\n",
    "        temp_loss+=loss.item()\n",
    "    print(\"ValidationLoss: \",temp_loss/len(valid_dataloader))\n",
    "    if temp_loss<avg_loss:\n",
    "        avg_loss=temp_loss\n",
    "        torch.save(model,\"face_feature.h\")\n",
    "    print(\"Training Epoch: \", epochs+1,\"\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss=1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

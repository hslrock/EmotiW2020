{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from constant import AFFECTNETPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file,base_path,transform=None):\n",
    "        self.fields = ['subDirectory_filePath', 'expression','valence','arousal']\n",
    "        self._table = pd.read_csv(csv_file,usecols=self.fields)\n",
    "        \n",
    "        self._table=self._table[self._table['expression'] <9]\n",
    "\n",
    "        self._table=self._table.reset_index(drop=True)\n",
    "\n",
    "        self._base_path=base_path\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.Resize((64,64)),\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path,self._table.subDirectory_filePath[idx])\n",
    "        img=Image.open(folder_name)\n",
    "        img=self.transform(img)\n",
    "        valence = torch.from_numpy(np.array(self._table.valence[idx]))\n",
    "        arousal = torch.from_numpy(np.array(self._table.arousal[idx]))\n",
    "        return img,valence.float(),arousal.float()\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Affect_Train=ImageDataset('training.csv',AFFECTNETPATH)\n",
    "Affect_Valid=ImageDataset('validation.csv',AFFECTNETPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320740"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Affect_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Affect_Valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    for i,file in enumerate(Affect_Valid):\n",
    "        if i%10000==0:\n",
    "            print(file[1])\n",
    "except FileNotFoundError:\n",
    "    \n",
    "    print(\"Error Occurs in:\", i)\n",
    "    pass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    for i,file in enumerate(Affect_Train):\n",
    "        if i%10000==0:\n",
    "            print(file[1])\n",
    "except FileNotFoundError:\n",
    "    \n",
    "    print(\"Error Occurs in:\", i)\n",
    "    pass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Affect_Train[243557][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(Affect_Train, batch_size=256\n",
    "                       , num_workers=0,shuffle=True)\n",
    "\n",
    "valid_dataloader = DataLoader(Affect_Valid, batch_size=256\n",
    "                       , num_workers=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1253"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Affect_Train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import face_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=face_feature.Face_Feature()\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3269]], grad_fn=<TanhBackward>),\n",
       " tensor([[0.3715]], grad_fn=<TanhBackward>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Affect_Train[0][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:  1 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  1 / 1253\n",
      "MSE loss:  0.5873039960861206\n",
      "Batch:  201 / 1253\n",
      "MSE loss:  0.6043837070465088\n"
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "num_epochs=50\n",
    "\n",
    "model.train()\n",
    "for epochs in range(0,num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    print(\"Training Epoch: \", epochs+1,\"\\n\")\n",
    "    for i_batch, (img,valence,arousal) in enumerate(train_dataloader):\n",
    "        \n",
    "        batch_size=img.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        img=img.to(device)\n",
    "        val_output,arou_output=model(img)\n",
    "        loss1=loss_func(val_output,valence.to(device))\n",
    "        loss2=loss_func(arou_output,valence.to(device))\n",
    "        loss=loss1+loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch%200==0:\n",
    "            print(\"Batch: \",i_batch+1,\"/\",len(train_dataloader))\n",
    "            print(\"MSE loss: \", loss.item())\n",
    "            train_loss.append(loss.item())\n",
    "    model.eval()\n",
    "    avg_loss=1e6\n",
    "    temp_loss=0\n",
    "    \n",
    "    for i_batch, (img,valence,arousal) in enumerate(train_dataloader):\n",
    "        \n",
    "        batch_size=img.size(0)\n",
    "\n",
    "        img=img.to(device)\n",
    "        val_output,arou_output=model(sample_batched)\n",
    "        loss1=loss_func(val_output,valence.to(device))\n",
    "        loss2=loss_func(arou_output,valence.to(device))\n",
    "        loss=loss1+loss2\n",
    "        temploss+=loss.item()\n",
    "    print(\"ValidationLoss: \",temp_loss/len(valid_dataloader))\n",
    "    if temp_loss<avg_loss:\n",
    "        avg_loss=temp_loss\n",
    "        torch.save(model,\"face_feature.h\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss=1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

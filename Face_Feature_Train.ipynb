{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from constant import AFFECTNETPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file,base_path,transform=None):\n",
    "        self.fields = ['subDirectory_filePath', 'expression','valence','arousal']\n",
    "        self._table = pd.read_csv(csv_file,usecols=self.fields)\n",
    "        \n",
    "        self._table=self._table[self._table['expression'] <8]\n",
    "\n",
    "        self._table=self._table.reset_index(drop=True)\n",
    "\n",
    "        self._base_path=base_path\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "        self.transform=transforms.Compose([\n",
    "                     transforms.Resize((32,32)),\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path,self._table.subDirectory_filePath[idx])\n",
    "        img=Image.open(folder_name)\n",
    "        img=self.transform(img)\n",
    "        valence = torch.from_numpy(np.array(self._table.valence[idx]))\n",
    "        return (img,valence.float())\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Affect_Train=ImageDataset('training.csv',AFFECTNETPATH)\n",
    "Affect_Valid=ImageDataset('validation.csv',AFFECTNETPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def import_data(filename):\n",
    "    \"\"\"Import data in the second column of the supplied filename as floats.\"\"\"\n",
    "    with open(filename, 'r') as inf:\n",
    "        inf.readline()\n",
    "        return [float(row[2]) for row in csv.reader(inf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287652"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Affect_Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(Affect_Train, batch_size=64\n",
    "                       , num_workers=0,shuffle=True)\n",
    "\n",
    "valid_dataloader = DataLoader(Affect_Valid, batch_size=64\n",
    "                       , num_workers=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4495"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Affect_Train[26][0].shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAJkklEQVR4nAXB2XNcV14A4PM7+126b2/qRS2pLUu2ZSeTrQJMUqSKguEF/hie5r+ZooowFA9QPFEUgSkoQiYzxiROmIzjyFZstWS11K1eb9/l3LPxffD4mzml1CFvMGIA7Yj0+wEjHjxyrlov15cXk6vx+OLVy/nsNiuyMs+rMiuzraABpcAD30zibqe3d3gcN9s7w/3ecI8HsUXYaessUG+scQ4BOIIwgTAQBLytyvH5+PHj//7qyTfnr6/STZaVqda6MsY4cMYQ8AyoRx4TTMAI4gWncSi7nc7RyaOPPvmzk3d+EoQ1AAyffzHGhADFgJjk5uhOPUuXn/3LP//q3z97PbnKVi43lfMVBewBPPYcvAQcEizBWe+0h61ShbXWIQxaMCGCcLe185OTh3/4yR+/9cF78Nm/PSeMUSoA+VYbFenF33/6y6f/9918vVmpTDjKOIukDJlkyAeMxRQ/6DZ3a7wWsMqYZZaPZzcXy9Uydzd5saxMaW1AREcG7Wbt7r1jijF2xmhnBXMvfjj7u09/cf56vNnm2thAsoNG2ApjLENjdJNCQ8g6o390ONzrx4I5TEmq9KYYTa6vJvPN08vpd9frN5ssV+qyKJfldl2UFLxDgMBVq8Xk07/565evxpvNNiCkLcPD3eFu4BpRfKuR05tRncdChJg0azhsBIR6ggltkFAVjcDtd5Oddr2ZLL/8/vWstGlVLUrlNilFyAAC5M3XT35zcfF6m+ecQ7dWOwiTo0ZYF3atNVbuqB4dtUPCZaVUJKkIJSKWUooBWSCsLkJGDygpgS032+JyhkRoM5xpTYOAeYAiS1+9PtVGq0qzkDKf7TWTQagopVleDjl5ONjpd2qEs9vFslIZBkSEdN4CAcIptcwDSRg+prhQg8lycbaqQi4LU9Jev8sEG7/aLBezUlWVMXmhgjg66MhePSCI1zjttOoHdwYy5hijMJLlammV9VwAIRgwIYhL75D2XnPIh/XgcKd7vpx4pxmjVEY8iCQgTYw2iAI45Fwjqd856LQbdVIZ6VCr2xWtBuLEaRsEJBcSee8BEclMVRqtMDBMQINyCBHkkyiKasF2tfKYUK0qSogzLpQiIGTrjEeAgHYGg067YVTlywIFEniAJXOodNaCFI6QsiiqdF1kmzzdChkFYegR9hgBg8pXDiyAU0pRZKypNOXiYH//ar7Ot6TUPldmawxebyXnhTZum5bWxM2EAl7cTi9eX41vbvPKgPfNetjv7ZReGesRQQYMCxgNKKe4wQUYRylhmOKkGZ88PDm7HDeboy+/+vF2o25ni8tVOl+m6+1GYLbbqL/37v3ewf74/Oar//16tsgcZigMiSCd21WMeaeZDI/6hHEhcbMmuoLGw966yqkMkOcQ1Wp7+7ve6b1+N4mvNll+M1sur6dZqbS1AdEpg3p7Nxncu/s2F0m4mS+Ex7LWAEGZBESJDOOoFW6ur5XaDuv1+52YMDaMdmgtEQYjZ1kQEFWpxXQdBaQqi/7u8P17d6PIhlxYF4T1pLGz7yFudgf1rnS2yJebbFU45MJmEnX3MJNFtqZkVZqUObeTiM5ed1Z6CrBkpE3BlcU6kKIZtw4PlFbbB4/e7nfa1qywc4yE1nuECfXeYvDAnGBxbad1IBFgYMgjmmdrn68rY9aq2KrceCtqkqo1Rf6KIrq6/vH7p1+C17PpbasTvvP+Ya0Vh+0uxXtW52YzVcs1D0OLC+czna1BMMKMQzlhwlkwqnLZqkrnZZat01TZynmokA8Fo2Bizy1ydj5ZeM+uZtOP7z1sJYJHFGHkisxlK72eocqQmrWusGqrszX1EoM0xiAjtKm2y6VSeZHnWZZVlcYYKwDnoRXFlPID512rd+zETrq9RRhubq777/X0alliSbK8vBmbbI2otCYDHBTpospWShGwIULeUV4U1XI58xgM8h6AcWbzChAgg02FqVZrhwhj7JM//8vfPT81lJRFiVE5ffkMjVzNVwpt0sXi1enl8O7cU3r24hnR6tH7byWJrEypdKENAkm9B+QQESoIQ7TIWRj855Nnv3s+oU5tgXCM+B98/NO/Ej//h7/9Zb65rHSBnVrNLngSilqCmhDW5v/z+ItSqeO7o9HDh81hzwLCNKQWAfGx4HmWu8oDRghAO4uFeHY+OVtvKY+GCICBRYB++id/eu+td37/9Ndo+usQaZCoxNZuc1Rke3udTrOmVbXTb7BO4sEVRWWtw8AQQYgiBIC8985b5CwFcJh6AdxTzzAgggCcQw6jpN/78JOfjR+P+fLKiTgri9uzMz/ZNHgoqeTebcYXZL5CVBbI5qZo97syDq1DzhiCOSVcV1suA5P5Jq9JgigCixBBgIFgDB55Q4GVesvdxusO84w7ow1gTySPPKWUNHVeZVo5ZkVAvLHeO51rAGSsLzRSlY2oqEjZvzOU2Tl98eJpkOw06kko6g4IQgQRkPUBtossnUei098drPXKa2zq0jPmK4NQ2WoJiDmNZW51qUpjPWG8Uk5pMBYT7Huj3qJJg1dT+vnn/9oeHR8MR6Pd4zBsYMAeFJU9WZuvJme3C5M0WDRI5mdvpi/fYCFlION6ELd2SI0bb4qV3Wy2jDMqoCwhXWRVqUVT7ux2UukH7QZN8zQ/fwGE5Uqlq63JTD0yPt8MAGGKZuO5svXeoN59+6hX2DCqkZADIxZ547Q2yhLlPfEWl6Vb3WZnz58f3h8290YySRr56sNHQ4qE2JbFb7/4Mk839bAJhsY1zZyeCPfuQS0M1TYt405HRlHUiHgQeLCEUVcpp3KjLUKcIFZu1Dxdvjg763SaeLDzH789fbDX2VYlBk6vb26zrHhzMTs6HFHA1zcLwaO9g92zHy4AZaM6KWbL1SwmmuJGXAECbx0Y631e6OV6m8/T5fV8ejMpnB08eDDemn/6x1/FWoTr9v5b7370wcdUBGGWFvdOjkNC3pydcRrvj+60O8l6VbycXJ/evjnpB6uz0ylbsFo9aHFBgDNhPcw3q9urK7PIrAfcqAGN/uvb84vJ9L7gb4/64U5jkZsmMPrNk28fPXy0eHMDAn9w77jW6B0d3nt+evnts9n49WtTKJYc9KLix+9PV6kyGPkgxpg568v1ptcOBrvdAuD0ejVf3Naz4i8Ou6POXiE6NzjKRPPr785pcb3xI80tSEyJTO6ffDidXqXbpLX7Po8Hi8nkfMOmKdOm2axvjEuRS0MZhkEgRt3rVP/mxdSzOIqbYX710dH+yf5wRXppdJB5cXb56offP/l/LiKNnd+fLbwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=32x32 at 0x7F8B92FA6E90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=transforms.ToPILImage()(Affect_Train[26][0]*0.5+0.5)\n",
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module import face_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "model=face_feature.Face_Feature()\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 16, 16]           9,408\n",
      "       BatchNorm2d-2           [-1, 64, 16, 16]             128\n",
      "              ReLU-3           [-1, 64, 16, 16]               0\n",
      "         MaxPool2d-4             [-1, 64, 8, 8]               0\n",
      "            Conv2d-5             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-6             [-1, 64, 8, 8]             128\n",
      "              ReLU-7             [-1, 64, 8, 8]               0\n",
      "            Conv2d-8             [-1, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9             [-1, 64, 8, 8]             128\n",
      "             ReLU-10             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-11             [-1, 64, 8, 8]               0\n",
      "           Conv2d-12             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-13             [-1, 64, 8, 8]             128\n",
      "             ReLU-14             [-1, 64, 8, 8]               0\n",
      "           Conv2d-15             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-16             [-1, 64, 8, 8]             128\n",
      "             ReLU-17             [-1, 64, 8, 8]               0\n",
      "       BasicBlock-18             [-1, 64, 8, 8]               0\n",
      "           Conv2d-19            [-1, 128, 4, 4]          73,728\n",
      "      BatchNorm2d-20            [-1, 128, 4, 4]             256\n",
      "             ReLU-21            [-1, 128, 4, 4]               0\n",
      "           Conv2d-22            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-23            [-1, 128, 4, 4]             256\n",
      "           Conv2d-24            [-1, 128, 4, 4]           8,192\n",
      "      BatchNorm2d-25            [-1, 128, 4, 4]             256\n",
      "             ReLU-26            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-27            [-1, 128, 4, 4]               0\n",
      "           Conv2d-28            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-29            [-1, 128, 4, 4]             256\n",
      "             ReLU-30            [-1, 128, 4, 4]               0\n",
      "           Conv2d-31            [-1, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-32            [-1, 128, 4, 4]             256\n",
      "             ReLU-33            [-1, 128, 4, 4]               0\n",
      "       BasicBlock-34            [-1, 128, 4, 4]               0\n",
      "           Conv2d-35            [-1, 512, 2, 2]         589,824\n",
      "      BatchNorm2d-36            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-37            [-1, 512, 2, 2]               0\n",
      "           Conv2d-38            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-39            [-1, 512, 2, 2]           1,024\n",
      "           Conv2d-40            [-1, 512, 2, 2]          65,536\n",
      "      BatchNorm2d-41            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-42            [-1, 512, 2, 2]               0\n",
      "       BasicBlock-43            [-1, 512, 2, 2]               0\n",
      "           Conv2d-44            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-45            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-46            [-1, 512, 2, 2]               0\n",
      "           Conv2d-47            [-1, 512, 2, 2]       2,359,296\n",
      "      BatchNorm2d-48            [-1, 512, 2, 2]           1,024\n",
      "             ReLU-49            [-1, 512, 2, 2]               0\n",
      "       BasicBlock-50            [-1, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-51            [-1, 512, 1, 1]               0\n",
      "           Linear-52                  [-1, 100]          51,300\n",
      "           ResNet-53                  [-1, 100]               0\n",
      "           Linear-54                    [-1, 1]             101\n",
      "             Tanh-55                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 8,472,841\n",
      "Trainable params: 8,472,841\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.35\n",
      "Params size (MB): 32.32\n",
      "Estimated Total Size (MB): 33.68\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3613]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(Affect_Train[0][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch:  1 \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0a8318eed14e7086b60d9848075623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  1 / 4495\n",
      "MSE loss:  0.2616032361984253\n",
      "Batch:  251 / 4495\n",
      "MSE loss:  0.2894498407840729\n",
      "Batch:  501 / 4495\n",
      "MSE loss:  0.29474470019340515\n",
      "Batch:  751 / 4495\n",
      "MSE loss:  0.2834385633468628\n",
      "Batch:  1001 / 4495\n",
      "MSE loss:  0.24670641124248505\n",
      "Batch:  1251 / 4495\n",
      "MSE loss:  0.3150092661380768\n",
      "Batch:  1501 / 4495\n",
      "MSE loss:  0.29874780774116516\n",
      "Batch:  1751 / 4495\n",
      "MSE loss:  0.23826083540916443\n",
      "Batch:  2001 / 4495\n",
      "MSE loss:  0.33283182978630066\n",
      "Batch:  2251 / 4495\n",
      "MSE loss:  0.2320093959569931\n",
      "Batch:  2501 / 4495\n",
      "MSE loss:  0.2953333556652069\n",
      "Batch:  2751 / 4495\n",
      "MSE loss:  0.27640900015830994\n",
      "Batch:  3001 / 4495\n",
      "MSE loss:  0.29040369391441345\n",
      "Batch:  3251 / 4495\n",
      "MSE loss:  0.2682853937149048\n",
      "Batch:  3501 / 4495\n",
      "MSE loss:  0.23518404364585876\n",
      "Batch:  3751 / 4495\n",
      "MSE loss:  0.2803976833820343\n",
      "Batch:  4001 / 4495\n",
      "MSE loss:  0.279130756855011\n",
      "Batch:  4251 / 4495\n",
      "MSE loss:  0.2473221719264984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([36])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ValidationLoss:  0.3586247061926221\n",
      "Training Epoch:  2 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/torch/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d32daa0380a4e4396cc772022cb92a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  1 / 4495\n",
      "MSE loss:  0.2544076442718506\n",
      "Batch:  251 / 4495\n",
      "MSE loss:  0.3091084957122803\n",
      "Batch:  501 / 4495\n",
      "MSE loss:  0.2346215844154358\n",
      "Batch:  751 / 4495\n",
      "MSE loss:  0.29820215702056885\n",
      "Batch:  1001 / 4495\n",
      "MSE loss:  0.2542441487312317\n",
      "Batch:  1251 / 4495\n",
      "MSE loss:  0.24393805861473083\n",
      "Batch:  1501 / 4495\n",
      "MSE loss:  0.258212685585022\n",
      "Batch:  1751 / 4495\n",
      "MSE loss:  0.2889578938484192\n",
      "Batch:  2001 / 4495\n",
      "MSE loss:  0.26300787925720215\n",
      "Batch:  2251 / 4495\n",
      "MSE loss:  0.2597668766975403\n",
      "Batch:  2501 / 4495\n",
      "MSE loss:  0.26610711216926575\n",
      "Batch:  2751 / 4495\n",
      "MSE loss:  0.2652439475059509\n",
      "Batch:  3001 / 4495\n",
      "MSE loss:  0.22981859743595123\n",
      "Batch:  3251 / 4495\n",
      "MSE loss:  0.27491411566734314\n"
     ]
    }
   ],
   "source": [
    "train_loss=[]\n",
    "val_loss=[]\n",
    "num_epochs=50\n",
    "\n",
    "for epochs in (range(0,num_epochs)):\n",
    "    model.train()\n",
    "    print(\"Training Epoch: \", epochs+1,\"\\n\")\n",
    "   \n",
    "    for i_batch, (img,valence) in tqdm(enumerate(train_dataloader)):\n",
    "        batch_size=img.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        img=img.to(device)\n",
    "        val_output=model(img)\n",
    "        loss1=loss_func(val_output,valence.to(device))\n",
    "        loss=loss1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i_batch%250==0:\n",
    "            print(\"Batch: \",i_batch+1,\"/\",len(train_dataloader))\n",
    "            print(\"MSE loss: \", loss.item())\n",
    "            train_loss.append(loss.item())\n",
    "    model.eval()\n",
    "    avg_loss=1e-6\n",
    "    temp_loss=0\n",
    "    for i_batch, (img,valence) in enumerate(valid_dataloader):\n",
    "        \n",
    "        batch_size=img.size(0)\n",
    "\n",
    "        img=img.to(device)\n",
    "        val_output=model(img)\n",
    "        loss1=loss_func(val_output,valence.to(device))\n",
    "        #loss2=loss_func(arou_output,valence.to(device))\n",
    "        loss=loss1\n",
    "        temp_loss+=loss.item()\n",
    "    print(\"ValidationLoss: \",temp_loss/len(valid_dataloader))\n",
    "    val_loss.append(temp_loss/len(valid_dataloader))\n",
    "    if temp_loss<avg_loss:\n",
    "        avg_loss=temp_loss\n",
    "        torch.save(model,\"face_feature.h\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss=1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

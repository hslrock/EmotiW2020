{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    932\n",
      "2    923\n",
      "1    806\n",
      "Name: Label, dtype: int64\n",
      "1    299\n",
      "2    281\n",
      "3    186\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Slef-made Function\n",
    "from Dataload import dataload\n",
    "#from module import transformer,video_model\n",
    "\n",
    "from module import video_model_attention\n",
    "from constant import EMOTIPATH,EMOTIFACEPATH,EMOTIAUDIOPATH\n",
    "from train import methods\n",
    "\n",
    "\n",
    "#Torch Library\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  utils\n",
    "\n",
    "#Sub tools\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Util Library\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid  \n",
    "\n",
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_video_pt=os.path.join(EMOTIPATH,\"pt\",\"Train\")\n",
    "Train_face_pt=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Train\")\n",
    "Audio_Train=os.path.join(EMOTIAUDIOPATH,\"training_audio_compare.csv\")\n",
    "\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_video_pt=os.path.join(EMOTIPATH,\"pt\",\"Valid\")\n",
    "Val_face_pt=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Valid\")\n",
    "Audio_Valid=os.path.join(EMOTIAUDIOPATH,\"validation_audio_compare.csv\")\n",
    "\n",
    "\n",
    "train_table = pd.read_csv(Train_label,delimiter=' ')\n",
    "val_table = pd.read_csv(Val_labels,delimiter=' ')\n",
    "print(train_table['Label'].value_counts())\n",
    "print(val_table['Label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "\n",
    "frame_num=10\n",
    "\n",
    "train_data_pt=dataload.Video_Frame_Data(Train_label,base_path_v=Train_video_pt,face_path=Train_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Train)\n",
    "valid_data_pt=dataload.Video_Frame_Data(Val_labels,base_path_v=Val_video_pt,face_path=Val_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Valid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained_model(pre_train):\n",
    "    \n",
    "    face_model=torch.load(\"pre_trained_model/\" +\"pre_trained_resnet18_facev2.h\")\n",
    "   # audio_model=video_model_attention.AudioRecognition(softmax=pre_train)\n",
    "   # audio_model.load_state_dict(torch.load(\"pre_trained_model/pre_embedded_audio.pth\"),strict=False)\n",
    "    image_model=video_model_attention.Video_modeller(frame_num,face_model=face_model,pre_train=pre_train)\n",
    "    image_model.load_state_dict(torch.load(\"global_image_train_v2/global_attetnion_9.pth\"),strict=False)\n",
    "    return(image_model)#,audio_model)\n",
    "\n",
    "\n",
    "\n",
    "def show_img(dataset,index,frame):\n",
    "    x=dataset[index][0][frame].cpu().numpy()\n",
    "    plt.figure(1,(15,15))\n",
    "    plt.axis('off')\n",
    "    image = (x*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    fig = plt.figure(2,(15,15))\n",
    "   # plt.axis('off')\n",
    "    grid = ImageGrid(fig, 111,\n",
    "                     nrows_ncols=(1,5),\n",
    "                     axes_pad=0.1,\n",
    "                     )\n",
    "    for i in range(5):\n",
    "        face=dataset[index][1][frame].cpu().numpy()\n",
    "        image = (face[i,:]*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "        grid[i].imshow(image,cmap='gray',interpolation='none')\n",
    "        \n",
    "img_model=load_pretrained_model(True)\n",
    "#audio_model=video_model_attention.AudioRecognition(softmax=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_model is frozen\n",
      "face_model is frozen\n",
      "en1 is unfrozen\n",
      "en2 is unfrozen\n",
      "fc1 is unfrozen\n",
      "fc2 is unfrozen\n",
      "fc3 is unfrozen\n",
      "dropout is unfrozen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weights=[932/932,932/923,932/806]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_data_pt, batch_size=32\n",
    "                       , num_workers=0,shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data_pt, batch_size=32\n",
    "                   , num_workers=0)\n",
    "#model=video_model.video_transformer(img_model,audio_model)\n",
    "model=img_model\n",
    "model=model.to(device)\n",
    "\n",
    "num_epochs=50\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  \n",
    "\n",
    "for name, child in model.named_children():\n",
    "    if not name in ['face_model','frame_model' ]:\n",
    "        print(name + ' is unfrozen')\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        print(name + ' is frozen')\n",
    "        for param in child.parameters():\n",
    "               param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1836, 0.7310, 0.0854]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_data_pt[6][0].unsqueeze(0).float().to(device),None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0772e-01, 8.9228e-01, 1.1791e-10]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(None,train_data_pt[0][1].unsqueeze(0).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_attention\n",
      "Epoch:  1 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.1753935913244884\n",
      "Validation Accuracy:  0.370757180156658\n",
      "Confusion Matrix: \n",
      " [[281   0  18]\n",
      " [255   0  26]\n",
      " [183   0   3]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  1.0187110900878906\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  1.0224113464355469\n",
      "Average_Loss:  1.0556310372693198\n",
      "Average_Accuracy:  0.4633596392333709\n",
      "Confusion Matrix: \n",
      " [[189 193 424]\n",
      " [134 347 442]\n",
      " [ 96 139 697]]\n",
      "Epoch:  2 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.1713650276263554\n",
      "Validation Accuracy:  0.360313315926893\n",
      "Confusion Matrix: \n",
      " [[ 13  62 224]\n",
      " [ 12 115 154]\n",
      " [ 12  26 148]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "model_name=\"image_attention\"\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "methods.image_train(\"image_train_v2\",num_epochs,model_name,model,train_dataloader,valid_dataloader,optimizer,criterion,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

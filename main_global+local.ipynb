{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    932\n",
      "2    923\n",
      "1    806\n",
      "Name: Label, dtype: int64\n",
      "1    299\n",
      "2    281\n",
      "3    186\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Slef-made Function\n",
    "from Dataload import dataload\n",
    "#from module import transformer,video_model\n",
    "\n",
    "from module import video_model_attention\n",
    "from constant import EMOTIPATH,EMOTIFACEPATH,EMOTIAUDIOPATH\n",
    "from train import methods\n",
    "\n",
    "\n",
    "#Torch Library\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  utils\n",
    "\n",
    "#Sub tools\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Util Library\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid  \n",
    "\n",
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_video_pt=os.path.join(EMOTIPATH,\"pt\",\"Train\")\n",
    "Train_face_pt=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Train\")\n",
    "Audio_Train=os.path.join(EMOTIAUDIOPATH,\"training_audio_compare.csv\")\n",
    "\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_video_pt=os.path.join(EMOTIPATH,\"pt\",\"Valid\")\n",
    "Val_face_pt=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Valid\")\n",
    "Audio_Valid=os.path.join(EMOTIAUDIOPATH,\"validation_audio_compare.csv\")\n",
    "\n",
    "\n",
    "train_table = pd.read_csv(Train_label,delimiter=' ')\n",
    "val_table = pd.read_csv(Val_labels,delimiter=' ')\n",
    "print(train_table['Label'].value_counts())\n",
    "print(val_table['Label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "\n",
    "frame_num=10\n",
    "\n",
    "train_data_pt=dataload.Video_Frame_Data(Train_label,base_path_v=Train_video_pt,face_path=Train_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Train)\n",
    "valid_data_pt=dataload.Video_Frame_Data(Val_labels,base_path_v=Val_video_pt,face_path=Val_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Valid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained_model(pre_train):\n",
    "    \n",
    "    face_model=torch.load(\"pre_trained_model/\" +\"pre_trained_resnet18_face.h\")\n",
    "   # audio_model=video_model_attention.AudioRecognition(softmax=pre_train)\n",
    "   # audio_model.load_state_dict(torch.load(\"pre_trained_model/pre_embedded_audio.pth\"),strict=False)\n",
    "    image_model=video_model_attention.Video_modeller(frame_num,face_model=face_model,pre_train=pre_train)\n",
    "    image_model.load_state_dict(torch.load(\"global_image_train_v2/global_attetnion_8.pth\"),strict=False)\n",
    "    return(image_model)#,audio_model)\n",
    "\n",
    "\n",
    "\n",
    "def show_img(dataset,index,frame):\n",
    "    x=dataset[index][0][frame].cpu().numpy()\n",
    "    plt.figure(1,(15,15))\n",
    "    plt.axis('off')\n",
    "    image = (x*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    fig = plt.figure(2,(15,15))\n",
    "   # plt.axis('off')\n",
    "    grid = ImageGrid(fig, 111,\n",
    "                     nrows_ncols=(1,5),\n",
    "                     axes_pad=0.1,\n",
    "                     )\n",
    "    for i in range(5):\n",
    "        face=dataset[index][1][frame].cpu().numpy()\n",
    "        image = (face[i,:]*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "        grid[i].imshow(image,cmap='gray',interpolation='none')\n",
    "        \n",
    "img_model=load_pretrained_model(True)\n",
    "#audio_model=video_model_attention.AudioRecognition(softmax=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_model is frozen\n",
      "face_model is frozen\n",
      "en1 is unfrozen\n",
      "en2 is unfrozen\n",
      "fc1 is unfrozen\n",
      "fc2 is unfrozen\n",
      "fc3 is unfrozen\n",
      "dropout is unfrozen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weights=[932/932,932/923,932/806]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_data_pt, batch_size=32\n",
    "                       , num_workers=0,shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data_pt, batch_size=32\n",
    "                   , num_workers=0)\n",
    "#model=video_model.video_transformer(img_model,audio_model)\n",
    "model=img_model\n",
    "model=model.to(device)\n",
    "\n",
    "num_epochs=50\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  \n",
    "\n",
    "for name, child in model.named_children():\n",
    "    if not name in ['face_model','frame_model' ]:\n",
    "        print(name + ' is unfrozen')\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        print(name + ' is frozen')\n",
    "        for param in child.parameters():\n",
    "               param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2625, 0.4401, 0.2974]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_data_pt[6][0].unsqueeze(0).float().to(device),None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3236, 0.3609, 0.3155]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(None,train_data_pt[0][1].unsqueeze(0).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train\n",
      "Epoch:  1 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.1875285853942235\n",
      "Validation Accuracy:  0.3577023498694517\n",
      "Confusion Matrix: \n",
      " [[  8 288   3]\n",
      " [ 17 264   0]\n",
      " [ 20 164   2]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  1.0178613662719727\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9569827318191528\n",
      "Average_Loss:  0.9460837244987488\n",
      "Average_Accuracy:  0.5836151822623074\n",
      "Confusion Matrix: \n",
      " [[396 201 209]\n",
      " [207 489 227]\n",
      " [126 138 668]]\n",
      "Epoch:  2 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  0.9960533653696378\n",
      "Validation Accuracy:  0.5313315926892951\n",
      "Confusion Matrix: \n",
      " [[164  69  66]\n",
      " [ 96 145  40]\n",
      " [ 50  38  98]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8847723603248596\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  1.1185576915740967\n",
      "Average_Loss:  0.9085966916311354\n",
      "Average_Accuracy:  0.6305900037579857\n",
      "Confusion Matrix: \n",
      " [[437 221 148]\n",
      " [166 570 187]\n",
      " [110 151 671]]\n",
      "Epoch:  3 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0204966813325882\n",
      "Validation Accuracy:  0.5091383812010444\n",
      "Confusion Matrix: \n",
      " [[170  35  94]\n",
      " [113 119  49]\n",
      " [ 52  33 101]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8696838021278381\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9584877490997314\n",
      "Average_Loss:  0.884813510236286\n",
      "Average_Accuracy:  0.6531379180759113\n",
      "Confusion Matrix: \n",
      " [[434 216 156]\n",
      " [145 606 172]\n",
      " [ 80 154 698]]\n",
      "Epoch:  4 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0529269029696782\n",
      "Validation Accuracy:  0.4660574412532637\n",
      "Confusion Matrix: \n",
      " [[ 51 117 131]\n",
      " [ 29 163  89]\n",
      " [ 16  27 143]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.781867504119873\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9810587167739868\n",
      "Average_Loss:  0.875613652524494\n",
      "Average_Accuracy:  0.6666666666666666\n",
      "Confusion Matrix: \n",
      " [[435 208 163]\n",
      " [138 614 171]\n",
      " [ 92 115 725]]\n",
      "Epoch:  5 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0607237642010052\n",
      "Validation Accuracy:  0.46344647519582244\n",
      "Confusion Matrix: \n",
      " [[ 56 136 107]\n",
      " [ 40 182  59]\n",
      " [ 28  41 117]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.9735804200172424\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8501314520835876\n",
      "Average_Loss:  0.8861463729824338\n",
      "Average_Accuracy:  0.6512589252160842\n",
      "Confusion Matrix: \n",
      " [[448 213 145]\n",
      " [156 583 184]\n",
      " [101 129 702]]\n",
      "Epoch:  6 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.2576033274332683\n",
      "Validation Accuracy:  0.283289817232376\n",
      "Confusion Matrix: \n",
      " [[ 52  31 216]\n",
      " [ 18  27 236]\n",
      " [ 29  19 138]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.9277234077453613\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9782190918922424\n",
      "Average_Loss:  0.8850573209070024\n",
      "Average_Accuracy:  0.6523863209319805\n",
      "Confusion Matrix: \n",
      " [[435 212 159]\n",
      " [157 585 181]\n",
      " [ 96 120 716]]\n",
      "Epoch:  7 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0720058207710583\n",
      "Validation Accuracy:  0.45430809399477806\n",
      "Confusion Matrix: \n",
      " [[131  19 149]\n",
      " [110  74  97]\n",
      " [ 39   4 143]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8850264549255371\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9259779453277588\n",
      "Average_Loss:  0.8892365921111334\n",
      "Average_Accuracy:  0.6501315295001879\n",
      "Confusion Matrix: \n",
      " [[422 221 163]\n",
      " [140 606 177]\n",
      " [ 94 136 702]]\n",
      "Epoch:  8 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0385614757736523\n",
      "Validation Accuracy:  0.49477806788511747\n",
      "Confusion Matrix: \n",
      " [[116 135  48]\n",
      " [ 65 163  53]\n",
      " [ 35  51 100]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.7596510648727417\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.842180609703064\n",
      "Average_Loss:  0.8879608313242594\n",
      "Average_Accuracy:  0.64750093949643\n",
      "Confusion Matrix: \n",
      " [[456 212 138]\n",
      " [175 569 179]\n",
      " [ 90 144 698]]\n",
      "Epoch:  9 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  0.9911276896794637\n",
      "Validation Accuracy:  0.5339425587467362\n",
      "Confusion Matrix: \n",
      " [[149 128  22]\n",
      " [ 89 151  41]\n",
      " [ 49  28 109]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  1.0054876804351807\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8169028759002686\n",
      "Average_Loss:  0.9094524426119668\n",
      "Average_Accuracy:  0.6298384066140549\n",
      "Confusion Matrix: \n",
      " [[415 221 170]\n",
      " [165 574 184]\n",
      " [113 132 687]]\n",
      "Epoch:  10 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0893275861938794\n",
      "Validation Accuracy:  0.4490861618798956\n",
      "Confusion Matrix: \n",
      " [[173  27  99]\n",
      " [106  78  97]\n",
      " [ 88   5  93]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.9410211443901062\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8985221982002258\n",
      "Average_Loss:  0.8928866471563067\n",
      "Average_Accuracy:  0.6437429537767756\n",
      "Confusion Matrix: \n",
      " [[441 203 162]\n",
      " [158 546 219]\n",
      " [ 88 118 726]]\n",
      "Epoch:  11 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0709113329648972\n",
      "Validation Accuracy:  0.45300261096605743\n",
      "Confusion Matrix: \n",
      " [[130  36 133]\n",
      " [106  86  89]\n",
      " [ 38  17 131]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8577334880828857\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8184874057769775\n",
      "Average_Loss:  0.8993221727155504\n",
      "Average_Accuracy:  0.6392333709131905\n",
      "Confusion Matrix: \n",
      " [[392 255 159]\n",
      " [129 613 181]\n",
      " [ 91 145 696]]\n",
      "Epoch:  12 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.062110185623169\n",
      "Validation Accuracy:  0.46736292428198434\n",
      "Confusion Matrix: \n",
      " [[ 68  87 144]\n",
      " [ 33 128 120]\n",
      " [ 17   7 162]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.9294142127037048\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.7308722138404846\n",
      "Average_Loss:  0.9034273198672703\n",
      "Average_Accuracy:  0.631717399473882\n",
      "Confusion Matrix: \n",
      " [[404 218 184]\n",
      " [162 551 210]\n",
      " [ 88 118 726]]\n",
      "Epoch:  13 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0506154124935467\n",
      "Validation Accuracy:  0.49477806788511747\n",
      "Confusion Matrix: \n",
      " [[174  54  71]\n",
      " [ 94 114  73]\n",
      " [ 48  47  91]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8585544228553772\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9371710419654846\n",
      "Average_Loss:  0.9179594892831076\n",
      "Average_Accuracy:  0.6174370537391958\n",
      "Confusion Matrix: \n",
      " [[379 197 230]\n",
      " [144 533 246]\n",
      " [ 96 105 731]]\n",
      "Epoch:  14 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.059878242512544\n",
      "Validation Accuracy:  0.47911227154047\n",
      "Confusion Matrix: \n",
      " [[209  15  75]\n",
      " [159  47  75]\n",
      " [ 63  12 111]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  1.0189788341522217\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.855801522731781\n",
      "Average_Loss:  0.9182653192962918\n",
      "Average_Accuracy:  0.6208192408868847\n",
      "Confusion Matrix: \n",
      " [[404 215 187]\n",
      " [183 552 188]\n",
      " [ 86 150 696]]\n",
      "Epoch:  15 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0476287727554638\n",
      "Validation Accuracy:  0.48825065274151436\n",
      "Confusion Matrix: \n",
      " [[139  37 123]\n",
      " [100  92  89]\n",
      " [ 34   9 143]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8361813426017761\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.976813018321991\n",
      "Average_Loss:  0.9146715658051627\n",
      "Average_Accuracy:  0.6272078166102969\n",
      "Confusion Matrix: \n",
      " [[431 178 197]\n",
      " [193 511 219]\n",
      " [ 96 109 727]]\n",
      "Epoch:  16 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.101892923315366\n",
      "Validation Accuracy:  0.4347258485639687\n",
      "Confusion Matrix: \n",
      " [[ 28 116 155]\n",
      " [ 21 159 101]\n",
      " [ 10  30 146]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.9541481137275696\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9881280064582825\n",
      "Average_Loss:  0.9274803847074509\n",
      "Average_Accuracy:  0.6133032694475761\n",
      "Confusion Matrix: \n",
      " [[359 272 175]\n",
      " [149 596 178]\n",
      " [ 78 177 677]]\n",
      "Epoch:  17 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.1323568895459175\n",
      "Validation Accuracy:  0.40339425587467365\n",
      "Confusion Matrix: \n",
      " [[  0 100 199]\n",
      " [  2 146 133]\n",
      " [  1  22 163]]\n",
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8565043807029724\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  1.0214232206344604\n",
      "Average_Loss:  0.9906113183214551\n",
      "Average_Accuracy:  0.5512965050732808\n",
      "Confusion Matrix: \n",
      " [[224 357 225]\n",
      " [122 595 206]\n",
      " [ 83 201 648]]\n",
      "Epoch:  18 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.2903393854697545\n",
      "Validation Accuracy:  0.2454308093994778\n",
      "Confusion Matrix: \n",
      " [[  1   0 298]\n",
      " [  1   1 279]\n",
      " [  0   0 186]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1b39e6851e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmethods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image_train_v2\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/notebooks/Github/EmotiW2020/train/methods.py\u001b[0m in \u001b[0;36mimage_train\u001b[0;34m(master_path, num_epochs, name, model, train_dataloader, valid_dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mavg_tloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mface_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/notebooks/Github/EmotiW2020/Dataload/dataload.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mvideo_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfolder_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mtemp_frame_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe_num\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_frame_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'encoding'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "model_name=\"image_train\"\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "methods.image_train(\"image_train_v2\",num_epochs,model_name,model,train_dataloader,valid_dataloader,optimizer,criterion,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

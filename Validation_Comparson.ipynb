{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def num_correct(prediction,labels):\n",
    "    correct=0\n",
    "    for i,(pred_label,label) in enumerate(zip(prediction,labels)):\n",
    "        if (pred_label.item()==label.item()):\n",
    "            correct +=1\n",
    "    return correct\n",
    "\n",
    "\n",
    "def evaluate(model,label=None):\n",
    "    model.eval()\n",
    "    print(\"Validation \\n\")\n",
    "    weights=[932/932,932/923,932/806]\n",
    "    class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)  \n",
    "    \n",
    "    correct=0\n",
    "    total_samples=0\n",
    "    first=True\n",
    "    label_list=[]\n",
    "    names=[]\n",
    "    for i_batch, (file_name,frame_batch,face_batch,audio_feature,label) in enumerate(valid_dataloader):\n",
    "        batch_size=audio_feature.size(0)\n",
    "        face_batch=face_batch.float().to(device)\n",
    "        frame_batch=frame_batch.float().to(device)\n",
    "        audio_feature=audio_feature.float().to(device)\n",
    "        output=model(frame_batch,face_batch,audio_feature)\n",
    "        predicted = torch.max(output, 1)\n",
    "        prediction=predicted.indices.detach().cpu()\n",
    "        if label is not None:\n",
    "            correct +=num_correct(prediction,label)\n",
    "        total_samples+=batch_size\n",
    "        true_label=label.detach().cpu()\n",
    "        label_list.append(true_label)\n",
    "        names.append(file_name)\n",
    "        if first:\n",
    "            first=False\n",
    "            conf_mat=confusion_matrix( true_label,prediction,labels=[0,1,2])\n",
    "        else:\n",
    "            conf_mat+=confusion_matrix(true_label,prediction,labels=[0,1,2])\n",
    "\n",
    "    avg_vaccuracy=correct/(total_samples)\n",
    "    print(\"Validation Accuracy: \", avg_vaccuracy)\n",
    "    print('Confusion Matrix: \\n',conf_mat)          \n",
    "    return names,label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slef-made Function\n",
    "from Dataload import dataload\n",
    "from module import transformer,video_model\n",
    "from constant import EMOTIPATH,EMOTIFACEPATH,EMOTIAUDIOPATH\n",
    "from train import methods\n",
    "\n",
    "\n",
    "#Torch Library\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  utils\n",
    "\n",
    "#Sub tools\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Util Library\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath=\"D:\\Dataset\\D-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath=\"D:\\Dataset\\D-001\"\n",
    "Val_labels=os.path.join(basepath,\"Val_labels.txt\")\n",
    "Val_video_pt=os.path.join(\"D:\\Dataset\\D-001\\images\",\"Valid\")\n",
    "Val_face_pt=os.path.join(\"D:\\Dataset\\D-001\\Face_Cropped\",\"Valid\")\n",
    "Audio_Valid=os.path.join(basepath,\"Val_audio.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_num=10\n",
    "valid_data_pt=dataload.Video_Frame_Data(Val_labels,base_path_v=Val_video_pt,face_path=Val_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(pre_train):\n",
    "    \n",
    "    audio_model=video_model.AudioRecognition(softmax=pre_train)\n",
    "    image_model=video_model.Video_modeller(frame_num,pre_train=pre_train)\n",
    "    return(image_model,audio_model)\n",
    "\n",
    "\n",
    "\n",
    "def show_img(dataset,index,frame):\n",
    "    x=dataset[index][0][frame].cpu().numpy()\n",
    "    plt.figure(1,(15,15))\n",
    "    plt.axis('off')\n",
    "    image = (x*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    fig = plt.figure(2,(15,15))\n",
    "   # plt.axis('off')\n",
    "    grid = ImageGrid(fig, 111,\n",
    "                     nrows_ncols=(1,5),\n",
    "                     axes_pad=0.1,\n",
    "                     )\n",
    "    for i in range(5):\n",
    "        face=dataset[index][1][frame].cpu().numpy()\n",
    "        image = (face[i,:]*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "        grid[i].imshow(image,cmap='gray',interpolation='none')\n",
    "        \n",
    "img_model,audio_model=load_pretrained_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1_1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_pt[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 3, 64, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_pt[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "valid_dataloader = DataLoader(valid_data_pt, batch_size=16\n",
    "                   , num_workers=0)\n",
    "\n",
    "model=video_model.video_transformer(img_model,audio_model)\n",
    "model=model.to(device)\n",
    "model.load_state_dict(torch.load(\"saved_final-002/full_v1_3.pth\"),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list=[]\n",
    "for i in range(49):\n",
    "    file_list.append(\"full_v1_\"+str(i+1)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_v1_1.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.54177545691906\n",
      "Confusion Matrix: \n",
      " [[169  82  51]\n",
      " [ 86 153  41]\n",
      " [ 28  63  93]]\n",
      "full_v1_2.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5639686684073107\n",
      "Confusion Matrix: \n",
      " [[132  96  74]\n",
      " [ 60 165  55]\n",
      " [ 12  37 135]]\n",
      "full_v1_3.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5182767624020888\n",
      "Confusion Matrix: \n",
      " [[185  54  63]\n",
      " [127 102  51]\n",
      " [ 41  33 110]]\n",
      "full_v1_4.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5704960835509139\n",
      "Confusion Matrix: \n",
      " [[164  66  72]\n",
      " [ 72 131  77]\n",
      " [ 16  26 142]]\n",
      "full_v1_5.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5339425587467362\n",
      "Confusion Matrix: \n",
      " [[167  75  60]\n",
      " [108 131  41]\n",
      " [ 25  48 111]]\n",
      "full_v1_6.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5065274151436031\n",
      "Confusion Matrix: \n",
      " [[123  80  99]\n",
      " [ 93 132  55]\n",
      " [ 21  30 133]]\n",
      "full_v1_7.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5221932114882507\n",
      "Confusion Matrix: \n",
      " [[ 73 172  57]\n",
      " [ 25 211  44]\n",
      " [  5  63 116]]\n",
      "full_v1_8.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5496083550913838\n",
      "Confusion Matrix: \n",
      " [[149  62  91]\n",
      " [ 76 127  77]\n",
      " [ 17  22 145]]\n",
      "full_v1_9.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5078328981723238\n",
      "Confusion Matrix: \n",
      " [[104 101  97]\n",
      " [ 49 145  86]\n",
      " [  9  35 140]]\n",
      "full_v1_10.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.4960835509138381\n",
      "Confusion Matrix: \n",
      " [[151  58  93]\n",
      " [105 104  71]\n",
      " [ 39  20 125]]\n",
      "full_v1_11.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5169712793733682\n",
      "Confusion Matrix: \n",
      " [[145  65  92]\n",
      " [ 77 119  84]\n",
      " [ 14  38 132]]\n",
      "full_v1_12.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5548302872062664\n",
      "Confusion Matrix: \n",
      " [[199  90  13]\n",
      " [119 149  12]\n",
      " [ 66  41  77]]\n",
      "full_v1_13.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5143603133159269\n",
      "Confusion Matrix: \n",
      " [[209  39  54]\n",
      " [164  78  38]\n",
      " [ 49  28 107]]\n",
      "full_v1_14.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.46736292428198434\n",
      "Confusion Matrix: \n",
      " [[ 99  65 138]\n",
      " [ 75 113  92]\n",
      " [ 16  22 146]]\n",
      "full_v1_15.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5443864229765013\n",
      "Confusion Matrix: \n",
      " [[144  96  62]\n",
      " [ 86 161  33]\n",
      " [ 38  34 112]]\n",
      "full_v1_16.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.4699738903394256\n",
      "Confusion Matrix: \n",
      " [[ 96  60 146]\n",
      " [ 48 112 120]\n",
      " [  7  25 152]]\n",
      "full_v1_17.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5248041775456919\n",
      "Confusion Matrix: \n",
      " [[135 107  60]\n",
      " [ 81 173  26]\n",
      " [ 34  56  94]]\n",
      "full_v1_18.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5496083550913838\n",
      "Confusion Matrix: \n",
      " [[169  80  53]\n",
      " [ 85 143  52]\n",
      " [ 34  41 109]]\n",
      "full_v1_19.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5313315926892951\n",
      "Confusion Matrix: \n",
      " [[130 102  70]\n",
      " [ 72 165  43]\n",
      " [ 27  45 112]]\n",
      "full_v1_20.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5052219321148825\n",
      "Confusion Matrix: \n",
      " [[ 91 181  30]\n",
      " [ 43 209  28]\n",
      " [ 23  74  87]]\n",
      "full_v1_21.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5078328981723238\n",
      "Confusion Matrix: \n",
      " [[153  66  83]\n",
      " [ 98 118  64]\n",
      " [ 46  20 118]]\n",
      "full_v1_22.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5013054830287206\n",
      "Confusion Matrix: \n",
      " [[133  54 115]\n",
      " [ 69 118  93]\n",
      " [ 38  13 133]]\n",
      "full_v1_23.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5548302872062664\n",
      "Confusion Matrix: \n",
      " [[151  68  83]\n",
      " [ 58 155  67]\n",
      " [ 25  40 119]]\n",
      "full_v1_24.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5274151436031331\n",
      "Confusion Matrix: \n",
      " [[115 105  82]\n",
      " [ 52 164  64]\n",
      " [ 17  42 125]]\n",
      "full_v1_25.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5496083550913838\n",
      "Confusion Matrix: \n",
      " [[124 116  62]\n",
      " [ 62 176  42]\n",
      " [ 18  45 121]]\n",
      "full_v1_26.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5365535248041775\n",
      "Confusion Matrix: \n",
      " [[137  83  82]\n",
      " [ 71 143  66]\n",
      " [ 32  21 131]]\n",
      "full_v1_27.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.4869451697127937\n",
      "Confusion Matrix: \n",
      " [[ 85  97 120]\n",
      " [ 51 161  68]\n",
      " [ 27  30 127]]\n",
      "full_v1_28.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5234986945169713\n",
      "Confusion Matrix: \n",
      " [[105 106  91]\n",
      " [ 46 164  70]\n",
      " [ 18  34 132]]\n",
      "full_v1_29.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5156657963446475\n",
      "Confusion Matrix: \n",
      " [[ 85  95 122]\n",
      " [ 36 160  84]\n",
      " [ 15  19 150]]\n",
      "full_v1_30.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5039164490861618\n",
      "Confusion Matrix: \n",
      " [[106  98  98]\n",
      " [ 52 143  85]\n",
      " [ 27  20 137]]\n",
      "full_v1_31.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5496083550913838\n",
      "Confusion Matrix: \n",
      " [[187  94  21]\n",
      " [ 99 160  21]\n",
      " [ 63  47  74]]\n",
      "full_v1_32.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.47780678851174935\n",
      "Confusion Matrix: \n",
      " [[ 91  91 120]\n",
      " [ 48 136  96]\n",
      " [ 31  14 139]]\n",
      "full_v1_33.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5483028720626631\n",
      "Confusion Matrix: \n",
      " [[191  50  61]\n",
      " [117 111  52]\n",
      " [ 58   8 118]]\n",
      "full_v1_34.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5391644908616188\n",
      "Confusion Matrix: \n",
      " [[158  77  67]\n",
      " [100 141  39]\n",
      " [ 53  17 114]]\n",
      "full_v1_35.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5339425587467362\n",
      "Confusion Matrix: \n",
      " [[140  79  83]\n",
      " [ 77 148  55]\n",
      " [ 20  43 121]]\n",
      "full_v1_36.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5391644908616188\n",
      "Confusion Matrix: \n",
      " [[143 102  57]\n",
      " [ 80 152  48]\n",
      " [ 34  32 118]]\n",
      "full_v1_37.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.489556135770235\n",
      "Confusion Matrix: \n",
      " [[ 89 112 101]\n",
      " [ 57 145  78]\n",
      " [ 25  18 141]]\n",
      "full_v1_38.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.4765013054830287\n",
      "Confusion Matrix: \n",
      " [[ 81  78 143]\n",
      " [ 43 132 105]\n",
      " [ 17  15 152]]\n",
      "full_v1_39.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5391644908616188\n",
      "Confusion Matrix: \n",
      " [[165  64  73]\n",
      " [ 87 112  81]\n",
      " [ 35  13 136]]\n",
      "full_v1_40.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5065274151436031\n",
      "Confusion Matrix: \n",
      " [[135  73  94]\n",
      " [ 82 131  67]\n",
      " [ 45  17 122]]\n",
      "full_v1_41.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.45691906005221933\n",
      "Confusion Matrix: \n",
      " [[ 88  79 135]\n",
      " [ 60 117 103]\n",
      " [ 27  12 145]]\n",
      "full_v1_42.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.4856396866840731\n",
      "Confusion Matrix: \n",
      " [[ 75  87 140]\n",
      " [ 39 152  89]\n",
      " [ 18  21 145]]\n",
      "full_v1_43.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.47127937336814624\n",
      "Confusion Matrix: \n",
      " [[108  82 112]\n",
      " [ 66 128  86]\n",
      " [ 43  16 125]]\n",
      "full_v1_44.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5195822454308094\n",
      "Confusion Matrix: \n",
      " [[136  80  86]\n",
      " [ 81 141  58]\n",
      " [ 40  23 121]]\n",
      "full_v1_45.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5326370757180157\n",
      "Confusion Matrix: \n",
      " [[112 130  60]\n",
      " [ 62 180  38]\n",
      " [ 37  31 116]]\n",
      "full_v1_46.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5195822454308094\n",
      "Confusion Matrix: \n",
      " [[187  45  70]\n",
      " [118  94  68]\n",
      " [ 54  13 117]]\n",
      "full_v1_47.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5169712793733682\n",
      "Confusion Matrix: \n",
      " [[140  90  72]\n",
      " [ 89 150  41]\n",
      " [ 48  30 106]]\n",
      "full_v1_48.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5039164490861618\n",
      "Confusion Matrix: \n",
      " [[ 97 113  92]\n",
      " [ 42 170  68]\n",
      " [ 27  38 119]]\n",
      "full_v1_49.pth\n",
      "Validation \n",
      "\n",
      "Validation Accuracy:  0.5104438642297651\n",
      "Confusion Matrix: \n",
      " [[138  84  80]\n",
      " [ 78 134  68]\n",
      " [ 38  27 119]]\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "for i in file_list:\n",
    "    print(i)\n",
    "    if os.path.isfile(\"saved_final-002/\"+i):        \n",
    "        model.load_state_dict(torch.load(\"saved_final-002/\"+i),strict=False)\n",
    "        data.append(evaluate(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "order=np.array(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "order=order.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_order=[]\n",
    "for index in order:\n",
    "    for label in index:\n",
    "        file_order.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_order=np.array([file_order]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(file_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "for trial in data:\n",
    "        print(trial)\n",
    "        break\n",
    "\n",
    "#prediction=data[1][1][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=[]\n",
    "for i in os.listdir(\"saved_final-002\"):\n",
    "    if i.startswith(\"full_v1_\"):\n",
    "        model_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list=sorted(model_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

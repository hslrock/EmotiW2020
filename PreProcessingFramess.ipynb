{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataload import dataload\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pylab as plt\n",
    "from torchvision import  utils\n",
    "from constant import EMOTIPATH,EMOTIFACEPATH\n",
    "from src import detect_faces, show_bboxes\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_base_path=\"../../../hdd/Dataset/Dataset/Face_Cropped2/\"\n",
    "\n",
    "\n",
    "<!-- os.mkdir(new_base_path)\n",
    "\n",
    "os.mkdir(os.path.join(new_base_path,\"Train\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\",\"Train\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"Valid\"))\n",
    "os.mkdir(os.path.join(new_base_path,\"pt\",\"Valid\")   -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Raw_Data():\n",
    "    def __init__(self,csv_file,sub_csv_file=None,\n",
    "                 base_path_v=None,frame_num=25,embedding=False):\n",
    "        \n",
    "        self.max_frame_num=25\n",
    "        self._table = pd.read_csv(csv_file,delimiter=' ')\n",
    "        if sub_csv_file is None:\n",
    "            self._table_embedding=None\n",
    "        else:\n",
    "            self._table_embedding=pd.read_csv(sub_csv_file)\n",
    "        self.frame_num = frame_num\n",
    "        self._base_path_v=base_path_v\n",
    "        self.embedding=embedding\n",
    "        self.transform=transforms.Compose([\n",
    "                    transforms.Resize((256,256)),\n",
    "            \n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "        self.endPad=self.transform(Image.new(mode='RGB', size=(256,256), color=0))    \n",
    "\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self._table)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder_name = os.path.join(self._base_path_v,self._table.Vid_name[idx]) #base path\n",
    "        first=True          \n",
    "        frame_raw_list=os.listdir(folder_name)\n",
    "        frame_len=len(frame_raw_list)\n",
    "\n",
    "        frame_raw_list=sorted(frame_raw_list)\n",
    "       # print(frame_raw_list)\n",
    "        frame_list=[]\n",
    "        \n",
    "        if frame_len<self.frame_num:\n",
    "            for index_0 in range(frame_len):\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_0])               \n",
    "                tempimg=Image.open(frame_path)       \n",
    "                frame_list.append(self.transform(tempimg))\n",
    "            \n",
    "        else:    \n",
    "            frame_index=(np.linspace(0,frame_len-1,self.frame_num,dtype=int))\n",
    "\n",
    "            for index_2 in frame_index:\n",
    "                frame_path=os.path.join(folder_name,frame_raw_list[index_2])\n",
    "                tempimg=Image.open(frame_path)\n",
    "                frame_list.append(self.transform(tempimg))\n",
    "        while(len(frame_list)<self.frame_num):\n",
    "            frame_list.append(self.endPad)\n",
    "        frame_data=torch.stack(frame_list,dim=0)\n",
    "            \n",
    "        return (folder_name,frame_data,frame_raw_list)\n",
    "    \n",
    "\n",
    "def face_extraction(img,max_number):\n",
    "        def toonePIL(x,max_number):\n",
    "            new_im = Image.new('RGB', (64*max_number,64))\n",
    "\n",
    "            x_offset = 0\n",
    "            for im in x:\n",
    "                new_im.paste(im, (x_offset,0))\n",
    "                x_offset += im.size[0]\n",
    "            return new_im\n",
    "\n",
    "        bounding_boxes, landmarks = detect_faces(img)\n",
    "        img_list=[]\n",
    "        stacked_img=torch.zeros((5,3,64,64))\n",
    "        transform=transforms.Resize((64,64))\n",
    "        transform2=transforms.Compose([\n",
    "                     transforms.ToTensor(),   \n",
    "                     transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "        for box_index,(left,right,up,bottom,_) in enumerate(bounding_boxes):\n",
    "            cropped_img=img.crop((left,right,up,bottom))\n",
    "            cropped_img=transform(cropped_img)\n",
    "            img_list.append(cropped_img)\n",
    "            \n",
    "            stacked_img[box_index]=transform2(cropped_img)\n",
    "            if len(img_list)==max_number:\n",
    "                break\n",
    "        while len(img_list) !=max_number:\n",
    "            END_PAD= Image.new(mode = \"RGB\", size = (64, 64), color =(0, 0, 0))\n",
    "            img_list.append(END_PAD)\n",
    "        img=toonePIL(img_list,max_number)\n",
    "        pt=transform2(img)\n",
    "        return img,pt,stacked_img\n",
    "    \n",
    "    \n",
    "Test_label=os.path.join(EMOTIPATH,\"Test_Data.csv\")\n",
    "Test_video=os.path.join(EMOTIPATH,\"Test\")\n",
    "Test_Data=Raw_Data(Test_label,base_path_v=Test_video,frame_num=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frame(data,data_type=\"Test\"):\n",
    "    for i in tqdm(data):\n",
    "        video_name=os.path.basename(i[0])\n",
    "        file_name=video_name+'.pt'\n",
    "        save_path=os.path.join(EMOTIPATH,\"pt\",data_type)\n",
    "        if not os.path.isdir(save_path):\n",
    "                os.mkdir(save_path)\n",
    "\n",
    "        torch.save(i[1], os.path.join(save_path,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_face(data,save_pt_path,save_image_path):\n",
    "    for index in tqdm(data):\n",
    "        video_name,_,frame_list=index\n",
    "        \n",
    "        for frame_name in frame_list:\n",
    "        \n",
    "            path=os.path.join(video_name,frame_name)\n",
    "            img=Image.open(path)\n",
    "            try:\n",
    "                face_cropped,pt=face_extraction(img,5)\n",
    "            except:\n",
    "                face_cropped=Image.new(mode='RGB', size=(320,64), color=0)\n",
    "                pt=transform2(face_cropped)\n",
    "                print(video_name,frame_name)\n",
    "                continue\n",
    "            \n",
    "            if not os.path.isdir(save_image_path):\n",
    "                os.mkdir(save_image_path)\n",
    "            if not os.path.isdir(save_pt_path):\n",
    "                os.mkdir(save_pt_path)\n",
    "            face_cropped.save((os.path.join(save_image_path,frame_name)))\n",
    "            torch.save(pt,(os.path.join(save_pt_path,os.path.splitext(frame_name)[0]+'.pt')))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_face(data,data_type=\"Test\",face_num=5):\n",
    "    transformation=transforms.Compose([\n",
    "                 transforms.ToTensor(),   \n",
    "                 transforms.Normalize((0.5,0.5,0.5 ), (0.5, 0.5,0.5))])\n",
    "    for index in tqdm(data):\n",
    "        video_name,_,frame_list=index\n",
    "        frame_size=len(frame_list)\n",
    "        save_stacked_pt_path=os.path.join(EMOTIFACEPATH,\"pt_stacked\",data_type)\n",
    "        if not os.path.isdir(save_stacked_pt_path):\n",
    "                os.mkdir(save_stacked_pt_path)\n",
    "        stacked_face=torch.empty(size=(frame_size,face_num,3,64,64))\n",
    "        for frame_num,frame_name in enumerate(frame_list):\n",
    "\n",
    "            path=os.path.join(video_name,frame_name)\n",
    "            img=Image.open(path)\n",
    "            try:\n",
    "                face_cropped,pt,stacked_img=face_extraction(img,5)\n",
    "            except:\n",
    "                stacked_img=torch.zeros((5,3,64,64))\n",
    "                face_cropped=Image.new(mode='RGB', size=(320,64), color=0)\n",
    "                pt=transformation(face_cropped)\n",
    "                print(video_name,frame_name)\n",
    "                continue\n",
    "            stacked_face[frame_num]=stacked_img\n",
    "#             save_image_path=os.path.join(EMOTIFACEPATH,\"Test\",os.path.basename(video_name))\n",
    "#             save_pt_path=os.path.join(EMOTIFACEPATH,\"pt\",\"Test\",os.path.basename(video_name))\n",
    "\n",
    "#             if not os.path.isdir(save_image_path):\n",
    "#                 os.mkdir(save_image_path)\n",
    "#             if not os.path.isdir(save_pt_path):\n",
    "#                 os.mkdir(save_pt_path)\n",
    "#             face_cropped.save((os.path.join(save_image_path,frame_name)))\n",
    "#             torch.save(pt,(os.path.join(save_pt_path,os.path.splitext(frame_name)[0]+'.pt')))\n",
    "        torch.save(stacked_face,(os.path.join(save_stacked_pt_path,os.path.basename(video_name)+'.pt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

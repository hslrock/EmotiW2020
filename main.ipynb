{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'train.visualization_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5a21be472b66>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvideo_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mconstant\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEMOTIPATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEMOTIFACEPATH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEMOTIAUDIOPATH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmethods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\EmotiW2020\\train\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvisualization_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_bboxes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdetect_faces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'train.visualization_utils'"
     ]
    }
   ],
   "source": [
    "#Slef-made Function\n",
    "from Dataload import dataload\n",
    "from module import transformer,video_model\n",
    "from constant import EMOTIPATH,EMOTIFACEPATH,EMOTIAUDIOPATH\n",
    "from train import methods\n",
    "\n",
    "\n",
    "#Torch Library\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  utils\n",
    "\n",
    "#Sub tools\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Util Library\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid  \n",
    "\n",
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_video_pt=os.path.join(EMOTIPATH,\"pt\",\"Train\")\n",
    "Train_face_pt=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Train\")\n",
    "Audio_Train=os.path.join(EMOTIAUDIOPATH,\"Train_audio.csv\")\n",
    "\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_video_pt=os.path.join(EMOTIPATH,\"pt\",\"Valid\")\n",
    "Val_face_pt=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Valid\")\n",
    "Audio_Valid=os.path.join(EMOTIAUDIOPATH,\"Val_audio.csv\")\n",
    "\n",
    "\n",
    "train_table = pd.read_csv(Train_label,delimiter=' ')\n",
    "val_table = pd.read_csv(Val_labels,delimiter=' ')\n",
    "print(train_table['Label'].value_counts())\n",
    "print(val_table['Label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "\n",
    "frame_num=10\n",
    "\n",
    "train_data_pt=dataload.Video_Frame_Data(Train_label,base_path_v=Train_video_pt,face_path=Train_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Train)\n",
    "valid_data_pt=dataload.Video_Frame_Data(Val_labels,base_path_v=Val_video_pt,face_path=Val_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Valid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained_model(pre_train):\n",
    "    \n",
    "    face_model=torch.load(\"pre_trained_model/\" +\"pre_trained_resnet18_face.h\")\n",
    "    audio_model=video_model.AudioRecognition(softmax=pre_train)\n",
    "    audio_model.load_state_dict(torch.load(\"pre_trained_model/pre_embedded_audio.pth\"),strict=False)\n",
    "    image_model=video_model.Video_modeller(frame_num,face_model=face_model,pre_train=pre_train)\n",
    "    return(image_model,audio_model)\n",
    "\n",
    "\n",
    "\n",
    "def show_img(dataset,index,frame):\n",
    "    x=dataset[index][0][frame].cpu().numpy()\n",
    "    plt.figure(1,(15,15))\n",
    "    plt.axis('off')\n",
    "    image = (x*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    fig = plt.figure(2,(15,15))\n",
    "    plt.axis('off')\n",
    "    grid = ImageGrid(fig, 111,\n",
    "                     nrows_ncols=(1,5),\n",
    "                     axes_pad=0.1,\n",
    "                     )\n",
    "    for i in range(5):\n",
    "        face=dataset[index][1][frame].cpu().numpy()\n",
    "        image = (face[i,:]*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "        grid[i].imshow(image,cmap='gray',interpolation='none')\n",
    "img_model,audio_model=load_pretrained_model(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'frame_num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ef05464450a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvideo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVideo_modeller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mface_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mface_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpre_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'frame_num' is not defined"
     ]
    }
   ],
   "source": [
    "show_img(train_data_pt,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    932\n",
      "2    923\n",
      "1    806\n",
      "Name: Label, dtype: int64\n",
      "1    299\n",
      "2    281\n",
      "3    186\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weights=[932/932,932/923,932/806]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_data_pt, batch_size=32\n",
    "                       , num_workers=0,shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data_pt, batch_size=32\n",
    "                   , num_workers=0)\n",
    "\n",
    "model=audio_model.to(device)\n",
    "\n",
    "num_epochs=50\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  \n",
    "\n",
    "for name, child in model.named_children():\n",
    "    if not name in ['face_model','frame_model' ]:\n",
    "        print(name + ' is unfrozen')\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        print(name + ' is frozen')\n",
    "        for param in child.parameters():\n",
    "               param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_modelv1\n",
      "Validation\n",
      "\n",
      "Training Epoch:  1 \n",
      "\n",
      "[[120  91  88]\n",
      " [ 84 141  56]\n",
      " [ 46  51  89]]\n",
      "1.0756492887934048\n",
      "Accuracy:  0.45691906005221933\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7c02482f4c405dbdc09b086424f09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.005, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "model_name=\"audio_modelv1\"\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "methods.full_train(\"saved_final\",num_epochs,model_name,model,train_dataloader,valid_dataloader,optimizer,criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    932\n",
      "2    923\n",
      "1    806\n",
      "Name: Label, dtype: int64\n",
      "1    299\n",
      "2    281\n",
      "3    186\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Slef-made Function\n",
    "from Dataload import dataload\n",
    "#from module import transformer,video_model\n",
    "\n",
    "from module import video_model_attention\n",
    "from constant import EMOTIPATH,EMOTIFACEPATH,EMOTIAUDIOPATH\n",
    "from train import methods\n",
    "\n",
    "\n",
    "#Torch Library\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import  utils\n",
    "\n",
    "#Sub tools\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "#Util Library\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid  \n",
    "\n",
    "Train_label=os.path.join(EMOTIPATH,\"Train_labels.txt\")\n",
    "Train_video_pt=os.path.join(EMOTIPATH,\"pt\",\"Train\")\n",
    "Train_face_pt=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Train\")\n",
    "Audio_Train=os.path.join(EMOTIAUDIOPATH,\"training_audio_compare.csv\")\n",
    "\n",
    "Val_labels=os.path.join(EMOTIPATH,\"Val_labels.txt\")\n",
    "Val_video_pt=os.path.join(EMOTIPATH,\"pt\",\"Valid\")\n",
    "Val_face_pt=os.path.join(EMOTIFACEPATH,\"pt_stacked\",\"Valid\")\n",
    "Audio_Valid=os.path.join(EMOTIAUDIOPATH,\"validation_audio_compare.csv\")\n",
    "\n",
    "\n",
    "train_table = pd.read_csv(Train_label,delimiter=' ')\n",
    "val_table = pd.read_csv(Val_labels,delimiter=' ')\n",
    "print(train_table['Label'].value_counts())\n",
    "print(val_table['Label'].value_counts())\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "\n",
    "\n",
    "\n",
    "frame_num=10\n",
    "\n",
    "train_data_pt=dataload.Video_Frame_Data(Train_label,base_path_v=Train_video_pt,face_path=Train_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Train)\n",
    "valid_data_pt=dataload.Video_Frame_Data(Val_labels,base_path_v=Val_video_pt,face_path=Val_face_pt,frame_num=frame_num,direct=True,audio_csv=Audio_Valid)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pretrained_model(pre_train):\n",
    "    face_model=torch.load(\"pre_trained_model/\" +\"pre_trained_resnet18_face.h\")\n",
    "    \n",
    "   # face_model=torch.load(\"pre_trained_model/\" +\"pre_trained_resnet18_facev2.h\")\n",
    "   # audio_model=video_model_attention.AudioRecognition(softmax=pre_train)\n",
    "   # audio_model.load_state_dict(torch.load(\"pre_trained_model/pre_embedded_audio.pth\"),strict=False)\n",
    "    image_model=video_model_attention.Video_modeller(frame_num,face_model=face_model,pre_train=pre_train)\n",
    "   # image_model.load_state_dict(torch.load(\"saved_model_img/img_global_5.pth\"),strict=False)\n",
    "    return(image_model)#,audio_model)\n",
    "\n",
    "\n",
    "\n",
    "def show_img(dataset,index,frame):\n",
    "    x=dataset[index][0][frame].cpu().numpy()\n",
    "    plt.figure(1,(15,15))\n",
    "    plt.axis('off')\n",
    "    image = (x*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    fig = plt.figure(2,(15,15))\n",
    "   # plt.axis('off')\n",
    "    grid = ImageGrid(fig, 111,\n",
    "                     nrows_ncols=(1,5),\n",
    "                     axes_pad=0.1,\n",
    "                     )\n",
    "    for i in range(5):\n",
    "        face=dataset[index][1][frame].cpu().numpy()\n",
    "        image = (face[i,:]*0.5+0.5).transpose((1, 2, 0)).squeeze()\n",
    "        grid[i].imshow(image,cmap='gray',interpolation='none')\n",
    "        \n",
    "img_model=load_pretrained_model(True)\n",
    "#audio_model=video_model_attention.AudioRecognition(softmax=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame_model is frozen\n",
      "face_model is frozen\n",
      "en1 is unfrozen\n",
      "en2 is unfrozen\n",
      "fc1 is unfrozen\n",
      "fc2 is unfrozen\n",
      "fc3 is unfrozen\n",
      "dropout is unfrozen\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weights=[932/932,932/923,932/806]\n",
    "class_weights = torch.FloatTensor(weights).to(device)\n",
    "\n",
    "train_dataloader = DataLoader(train_data_pt, batch_size=32\n",
    "                       , num_workers=0,shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_data_pt, batch_size=32\n",
    "                   , num_workers=0)\n",
    "#model=video_model.video_transformer(img_model,audio_model)\n",
    "model=img_model\n",
    "model=model.to(device)\n",
    "\n",
    "num_epochs=50\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  \n",
    "\n",
    "for name, child in model.named_children():\n",
    "    if not name in ['face_model','frame_model' ]:\n",
    "        print(name + ' is unfrozen')\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        print(name + ' is frozen')\n",
    "        for param in child.parameters():\n",
    "               param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1466, 0.3838, 0.4696]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(train_data_pt[6][0].unsqueeze(0).float().to(device),None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1741, 0.3477, 0.4782]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(None,train_data_pt[0][1].unsqueeze(0).float().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_attetnion\n",
      "Epoch:  1 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.122681496044\n",
      "Validation Accuracy:  0.3968668407310705\n",
      "Confusion Matrix: \n",
      " [[  0 292   7]\n",
      " [  0 267  14]\n",
      " [  0 149  37]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ad91504cce74f40b047e786a0170940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  1.067987084388733\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  1.0076818466186523\n",
      "\n",
      "Average_Loss:  0.978890468676885\n",
      "Average_Accuracy:  0.5377677564825254\n",
      "Confusion Matrix: \n",
      " [[304 261 241]\n",
      " [203 480 240]\n",
      " [112 173 647]]\n",
      "Epoch:  2 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0212591538826625\n",
      "Validation Accuracy:  0.4960835509138381\n",
      "Confusion Matrix: \n",
      " [[164  39  96]\n",
      " [140  97  44]\n",
      " [ 23  44 119]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7101ba22b3547fdb34330d04aa5aba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8937707543373108\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  1.0243604183197021\n",
      "\n",
      "Average_Loss:  0.9255925913651785\n",
      "Average_Accuracy:  0.608793686583991\n",
      "Confusion Matrix: \n",
      " [[391 228 187]\n",
      " [171 552 200]\n",
      " [ 88 167 677]]\n",
      "Epoch:  3 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0105154166618984\n",
      "Validation Accuracy:  0.5052219321148825\n",
      "Confusion Matrix: \n",
      " [[174  55  70]\n",
      " [114 138  29]\n",
      " [ 47  64  75]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed8f7cb0ccc47828601af4f7e2d7855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8208170533180237\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9051774740219116\n",
      "\n",
      "Average_Loss:  0.8968622407742909\n",
      "Average_Accuracy:  0.6366027809094326\n",
      "Confusion Matrix: \n",
      " [[417 218 171]\n",
      " [177 584 162]\n",
      " [ 87 152 693]]\n",
      "Epoch:  4 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  0.9821802228689194\n",
      "Validation Accuracy:  0.5339425587467362\n",
      "Confusion Matrix: \n",
      " [[126  70 103]\n",
      " [ 63 138  80]\n",
      " [ 20  21 145]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56743e04b5d64a329c46bc72833a6731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8362656831741333\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9653229713439941\n",
      "\n",
      "Average_Loss:  0.876470535283997\n",
      "Average_Accuracy:  0.6557685080796692\n",
      "Confusion Matrix: \n",
      " [[459 184 163]\n",
      " [173 575 175]\n",
      " [ 98 123 711]]\n",
      "Epoch:  5 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.001938745379448\n",
      "Validation Accuracy:  0.5078328981723238\n",
      "Confusion Matrix: \n",
      " [[119  76 104]\n",
      " [ 87 134  60]\n",
      " [ 28  22 136]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea0d4dfbe514353b9b324ae80a4adb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  1.0016796588897705\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8131349682807922\n",
      "\n",
      "Average_Loss:  0.8697719737177804\n",
      "Average_Accuracy:  0.6677940623825629\n",
      "Confusion Matrix: \n",
      " [[456 195 155]\n",
      " [165 584 174]\n",
      " [ 85 110 737]]\n",
      "Epoch:  6 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  0.9879389926791191\n",
      "Validation Accuracy:  0.5469973890339426\n",
      "Confusion Matrix: \n",
      " [[146 117  36]\n",
      " [ 78 189  14]\n",
      " [ 50  52  84]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98db78f6b41241af8a654b52abacc84f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.9247068762779236\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9670823812484741\n",
      "\n",
      "Average_Loss:  0.8634782043241319\n",
      "Average_Accuracy:  0.6753100338218715\n",
      "Confusion Matrix: \n",
      " [[461 199 146]\n",
      " [143 612 168]\n",
      " [ 85 123 724]]\n",
      "Epoch:  7 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  0.9952866559227308\n",
      "Validation Accuracy:  0.5143603133159269\n",
      "Confusion Matrix: \n",
      " [[183  44  72]\n",
      " [135  91  55]\n",
      " [ 53  13 120]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340573ad3f0247d090a7002bd129595e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8637521862983704\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8911202549934387\n",
      "\n",
      "Average_Loss:  0.8546858032544454\n",
      "Average_Accuracy:  0.6907177752724539\n",
      "Confusion Matrix: \n",
      " [[481 196 129]\n",
      " [145 626 152]\n",
      " [ 82 119 731]]\n",
      "Epoch:  8 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  0.997890534500281\n",
      "Validation Accuracy:  0.5300261096605744\n",
      "Confusion Matrix: \n",
      " [[207  44  48]\n",
      " [147  90  44]\n",
      " [ 70   7 109]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cba80b871fe43139227c7af7ffbe77c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.73744797706604\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.7017444372177124\n",
      "\n",
      "Average_Loss:  0.8410401599747794\n",
      "Average_Accuracy:  0.6997369409996242\n",
      "Confusion Matrix: \n",
      " [[515 172 119]\n",
      " [155 613 155]\n",
      " [ 76 122 734]]\n",
      "Epoch:  9 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0017687504490216\n",
      "Validation Accuracy:  0.5287206266318538\n",
      "Confusion Matrix: \n",
      " [[159  93  47]\n",
      " [100 152  29]\n",
      " [ 75  17  94]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a714989fb21e43dd934ba67a16483cb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.9425851702690125\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8114534616470337\n",
      "\n",
      "Average_Loss:  0.8378681320519674\n",
      "Average_Accuracy:  0.703870725291244\n",
      "Confusion Matrix: \n",
      " [[521 164 121]\n",
      " [151 617 155]\n",
      " [ 93 104 735]]\n",
      "Epoch:  10 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.037238302330176\n",
      "Validation Accuracy:  0.47389033942558745\n",
      "Confusion Matrix: \n",
      " [[ 71 104 124]\n",
      " [ 45 154  82]\n",
      " [ 22  26 138]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a614293cff54918905d1d8982f4d31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.8506784439086914\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8821864724159241\n",
      "\n",
      "Average_Loss:  0.8266832644031161\n",
      "Average_Accuracy:  0.7177752724539647\n",
      "Confusion Matrix: \n",
      " [[502 183 121]\n",
      " [123 649 151]\n",
      " [ 65 108 759]]\n",
      "Epoch:  11 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.058776470522086\n",
      "Validation Accuracy:  0.4451697127937337\n",
      "Confusion Matrix: \n",
      " [[ 41 145 113]\n",
      " [ 47 182  52]\n",
      " [ 40  28 118]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d048687f72d4ca0bba4e1913d0575df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.7425724267959595\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.7960951924324036\n",
      "\n",
      "Average_Loss:  0.8238337359258107\n",
      "Average_Accuracy:  0.7113866967305524\n",
      "Confusion Matrix: \n",
      " [[496 197 113]\n",
      " [148 641 134]\n",
      " [ 57 119 756]]\n",
      "Epoch:  12 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.066548151274522\n",
      "Validation Accuracy:  0.4516971279373368\n",
      "Confusion Matrix: \n",
      " [[108  47 144]\n",
      " [ 99  93  89]\n",
      " [ 37   4 145]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acb5cf3a3fc4d6a94bc821b42468bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.9009593725204468\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.6980078816413879\n",
      "\n",
      "Average_Loss:  0.8141272791794368\n",
      "Average_Accuracy:  0.7234122510334461\n",
      "Confusion Matrix: \n",
      " [[522 178 106]\n",
      " [140 654 129]\n",
      " [ 66 117 749]]\n",
      "Epoch:  13 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  0.9636167958378792\n",
      "Validation Accuracy:  0.5704960835509139\n",
      "Confusion Matrix: \n",
      " [[139 111  49]\n",
      " [ 77 170  34]\n",
      " [ 31  27 128]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb041a58b054db8bd6cb03cda6eeeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.797113299369812\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8831629753112793\n",
      "\n",
      "Average_Loss:  0.8044029303959438\n",
      "Average_Accuracy:  0.7380683953400977\n",
      "Confusion Matrix: \n",
      " [[539 144 123]\n",
      " [132 653 138]\n",
      " [ 62  98 772]]\n",
      "Epoch:  14 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0173350845774014\n",
      "Validation Accuracy:  0.5078328981723238\n",
      "Confusion Matrix: \n",
      " [[144  88  67]\n",
      " [ 91 154  36]\n",
      " [ 72  23  91]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc93c692b184dd1aaea7a0afd9563ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.70585036277771\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.778419017791748\n",
      "\n",
      "Average_Loss:  0.7935718397299448\n",
      "Average_Accuracy:  0.7474633596392334\n",
      "Confusion Matrix: \n",
      " [[540 165 101]\n",
      " [130 661 132]\n",
      " [ 52  92 788]]\n",
      "Epoch:  15 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0626370410124462\n",
      "Validation Accuracy:  0.4595300261096606\n",
      "Confusion Matrix: \n",
      " [[113  80 106]\n",
      " [113 114  54]\n",
      " [ 53   8 125]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a91b523c8d5483c90af1b12afdba87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.758033812046051\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.8746766448020935\n",
      "\n",
      "Average_Loss:  0.7904762568927947\n",
      "Average_Accuracy:  0.7564825253664036\n",
      "Confusion Matrix: \n",
      " [[543 160 103]\n",
      " [110 676 137]\n",
      " [ 53  85 794]]\n",
      "Epoch:  16 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0584629277388256\n",
      "Validation Accuracy:  0.4516971279373368\n",
      "Confusion Matrix: \n",
      " [[ 88  63 148]\n",
      " [ 62 110 109]\n",
      " [ 27  11 148]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ad8b3d9d8d4dc3a2f53fa4a030dfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.7736694812774658\n",
      "Batch:  80 / 84\n",
      "Batch Recognition loss:  0.9468644261360168\n",
      "\n",
      "Average_Loss:  0.7950658635014579\n",
      "Average_Accuracy:  0.7485907553551296\n",
      "Confusion Matrix: \n",
      " [[549 153 104]\n",
      " [120 682 121]\n",
      " [ 62 109 761]]\n",
      "Epoch:  17 \n",
      "\n",
      "Validation \n",
      "\n",
      "Validation Loss:  1.0705550263325374\n",
      "Validation Accuracy:  0.4386422976501306\n",
      "Confusion Matrix: \n",
      " [[ 67  61 171]\n",
      " [ 39 107 135]\n",
      " [ 12  12 162]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a1e33f00dd4628bd59e13dc218593b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:  40 / 84\n",
      "Batch Recognition loss:  0.7546444535255432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-5-ecaa81700cf1>\", line 8, in <module>\n",
      "    methods.frame_train(\"global_image_train_v2\",num_epochs,model_name,model,train_dataloader,valid_dataloader,optimizer,criterion,device)\n",
      "  File \"/opt/notebooks/Github/EmotiW2020/train/methods.py\", line 144, in frame_train\n",
      "    for i_batch, (frame_batch,_,_,label) in tqdm(enumerate(train_dataloader)):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/tqdm/notebook.py\", line 215, in __iter__\n",
      "    for obj in super(tqdm_notebook, self).__iter__(*args, **kwargs):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/tqdm/std.py\", line 1097, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 346, in __next__\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/opt/notebooks/Github/EmotiW2020/Dataload/dataload.py\", line 51, in __getitem__\n",
      "    temp_face_data=torch.load(faces_path)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\", line 426, in load\n",
      "    return _load(f, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\", line 595, in _load\n",
      "    return legacy_load(f)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/torch/serialization.py\", line 506, in legacy_load\n",
      "    with closing(tarfile.open(fileobj=f, mode='r:', format=tarfile.PAX_FORMAT)) as tar, \\\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/tarfile.py\", line 1591, in open\n",
      "    return func(name, filemode, fileobj, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/tarfile.py\", line 1621, in taropen\n",
      "    return cls(name, mode, fileobj, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/tarfile.py\", line 1484, in __init__\n",
      "    self.firstmember = self.next()\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/tarfile.py\", line 2289, in next\n",
      "    tarinfo = self.tarinfo.fromtarfile(self)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/tarfile.py\", line 1094, in fromtarfile\n",
      "    buf = tarfile.fileobj.read(BLOCKSIZE)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/opt/conda/envs/torch/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "model_name=\"global_attetnion\"\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "methods.frame_train(\"global_image_train_v2\",num_epochs,model_name,model,train_dataloader,valid_dataloader,optimizer,criterion,device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
